# extraction/core_utils.py

import re
import os
import time
import psutil
import urllib.parse
import logging
import requests
import pandas as pd

from bs4 import BeautifulSoup
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from datetime import datetime
from urllib.parse import urlparse

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from dotenv import load_dotenv

# ì œì™¸ ë„ë©”ì¸ ë¡œì§ ì‚­ì œë¨

# ì´ˆê¸°í™”
today = datetime.now().strftime("%y%m%d")

# .env ë¡œë“œ
load_dotenv(dotenv_path="/opt/airflow/.env")

# ë¡œê·¸ ì €ì¥ ê²½ë¡œ ì„¤ì •
log_dir = "/opt/airflow/logs/extraction" 
os.makedirs(log_dir, exist_ok=True)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    filename=os.path.join(log_dir, "log.txt"),
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    encoding='utf-8'
)

# Requests ì„¸ì…˜ ìƒì„±
session = requests.Session()
session.headers.update({
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
})

def log(msg, index=None):
    prefix = f"[{index+1:03d}] " if index is not None else ""
    full_msg = f"{prefix}{msg}"
    print(full_msg)
    logging.info(full_msg)

def create_driver(index=None):
    try:
        options = Options()
        # Docker í™˜ê²½ ì„¤ì •
        options.add_argument("--headless=new")
        options.add_argument("--disable-background-networking")
        options.add_argument("--disable-sync")
        options.add_argument("--disable-default-apps")
        options.add_argument("--no-first-run")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-extensions")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_argument("--blink-settings=imagesEnabled=false")
        options.add_argument("--lang=ko_KR")
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

        prefs = {
            "profile.managed_default_content_settings.images": 2,
            "profile.default_content_setting_values.media_stream": 2,
            "profile.default_content_setting_values.notifications": 2,
            "profile.default_content_setting_values.geolocation": 2,
            "profile.default_content_setting_values.media_stream_mic": 2,
            "profile.default_content_setting_values.media_stream_camera": 2,
        }
        options.add_experimental_option("prefs", prefs)
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option("useAutomationExtension", False)
        options.page_load_strategy = 'eager'

        # Chromium ê²½ë¡œ ê³ ì •
        options.binary_location = "/usr/bin/chromium"
        
        # ì‹œìŠ¤í…œ ë“œë¼ì´ë²„ ìš°ì„  ì‚¬ìš©
        system_driver_path = "/usr/bin/chromedriver"
        
        if os.path.exists(system_driver_path):
            service = Service(executable_path=system_driver_path)
        else:
            # Fallback
            driver_path = os.getenv("CHROMEDRIVER_PATH")
            if driver_path and os.path.exists(driver_path):
                service = Service(executable_path=driver_path)
            else:
                # webdriver_manager fallback (if installed)
                from webdriver_manager.chrome import ChromeDriverManager
                service = Service(ChromeDriverManager().install())

        driver = webdriver.Chrome(service=service, options=options)

        driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
            "source": """
                Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
                Object.defineProperty(navigator, 'languages', { get: () => ['ko-KR', 'ko', 'en-US', 'en'] });
                Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5] });
            """
        })

        return driver
    except Exception as e:
        log(f"âŒ ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨: {e}", index)
        return None

def kill_driver(driver, index=None):
    if driver:
        try:
            driver.quit()
        except Exception:
            pass

        try:
            if hasattr(driver, 'service') and driver.service.process:
                pid = driver.service.process.pid
                if psutil.pid_exists(pid):
                    parent = psutil.Process(pid)
                    children = parent.children(recursive=True)
                    for child in children:
                        child.kill()
                    parent.kill()
        except Exception:
            pass

def clean_text(text):
    if not isinstance(text, str):
        text = str(text)
    if text.strip().lower() == 'nan':
        return ""

    patterns_to_remove = [
        r"Video Player", r"Video íƒœê·¸ë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¸Œë¼ìš°ì €ì…ë‹ˆë‹¤\.", 
        r"\d{2}:\d{2}", r"[01]\.\d{2}x", r"ì¶œì²˜:\s?[^\n]+", 
        r"/\s?\d+\.?\d*", r"Your browser does not support the video tag."
    ]
    for pattern in patterns_to_remove:
        text = re.sub(pattern, "", text)

    text = text.replace("\\\"", "\"").replace("\\'", "'").replace("\\\\", "\\")
    text = re.sub(r"[ã…‹ã…ã… ã…œ]+", "", text)
    text = re.sub(r"[!?~\.\,\-#]{2,}", "", text)
    text = re.sub(r"&[a-z]+;|&#\d+;", "", text)
    text = re.sub(r"[\\\xa0\u200b\u3000\u200c_x000D_]", " ", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()

def extract_keywords(text, num_keywords=5):
    try:
        from konlpy.tag import Okt
        local_okt = Okt() 
        nouns = local_okt.nouns(text)
        return " ".join(nouns[:num_keywords])
    except Exception as e:
        print(f"âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return ""

def extract_first_sentences(text):
    paras = re.split(r'\n{2,}', text.strip())
    get_first = lambda p: re.split(r'(?<=[.!?])(?=\s|[ê°€-í£])', p.strip())[0] if p else ""
    get_last = lambda p: re.split(r'(?<=[.!?])(?=\s|[ê°€-í£])', p.strip())[-1].strip() if p else ""
    
    first = get_first(paras[0]) if len(paras) > 0 else ""
    second = get_first(paras[1]) if len(paras) > 1 else ""
    last = get_last(paras[-1]) if len(paras) > 0 else ""
    return first, second, last

def calculate_copy_ratio(article, post):
    def clean(t): return re.sub(r'\s+', ' ', re.sub(r'[^\w\s]', '', t)).strip()
    article, post = clean(article), clean(post)
    
    if not article or not post:
        return 0.0

    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\s+', article) if s.strip()]
    if not sentences: return 0.0
    
    scores = []
    for s in sentences:
        try:
            if not s: continue
            v = TfidfVectorizer().fit([s, post])
            tfidf = v.transform([s, post])
            scores.append(cosine_similarity(tfidf[0:1], tfidf[1:2])[0][0])
        except:
            continue
    return round(sum(scores)/len(scores), 3) if scores else 0.0

def safe_get(driver, url, timeout=15, index=None):
    try:
        driver.set_page_load_timeout(timeout)
        driver.get(url)
        return True
    except Exception:
        return False

def fallback_with_requests(url):
    try:
        res = session.get(url, timeout=10)
        if res.status_code != 200:
            return ""
        
        soup = BeautifulSoup(res.text, "html.parser")
        
        # 1. ë„¤ì´ë²„ ë‰´ìŠ¤ ê³µí†µ ì…€ë ‰í„° ì‹œë„
        content_div = soup.select_one("#dic_area, ._article_content, #articleBody")
        if content_div:
            return content_div.get_text(strip=True)
            
        # 2. ë©”íƒ€ íƒœê·¸
        meta_content = soup.find("meta", {"name": "articleBody"})
        if meta_content:
            return meta_content.get("content", "").strip()

        # 3. ìµœí›„ì˜ ìˆ˜ë‹¨
        paragraphs = soup.find_all("p")
        return "\n".join(p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20)
    except:
        return ""

def get_news_article_body(url, driver, max_retries=1, index=None):
    try:
        # Selenium ì‹œë„
        if safe_get(driver, url, timeout=10, index=index):
            soup = BeautifulSoup(driver.page_source, "html.parser")
            domain = urllib.parse.urlparse(url).netloc

            # [ë³€ê²½] ë„¤ì´ë²„ ê´€ë ¨ ì…€ë ‰í„°ë§Œ ìœ ì§€
            selector_map = {
                "n.news.naver.com": "article#dic_area", # ì¼ë°˜ ë‰´ìŠ¤
                "m.sports.naver.com": "div._article_content", # ìŠ¤í¬ì¸ 
                "m.entertain.naver.com": "article#comp_news_article div._article_content", # ì—°ì˜ˆ
                "entertain.naver.com": "div.article_body", # ì—°ì˜ˆ (PC)
                "sports.news.naver.com": "div#newsEndContents", # ìŠ¤í¬ì¸  (PC)
            }
            
            # 1. ë„ë©”ì¸ë³„ ì§€ì • ì…€ë ‰í„° ê²€ìƒ‰
            selector = next((v for k, v in selector_map.items() if k in domain), None)
            if selector:
                div = soup.select_one(selector)
                if div:
                    body = div.get_text(separator="\n", strip=True)
                    if len(body) > 200:
                        return body, driver

            # 2. ê³µí†µ ì…€ë ‰í„° ê²€ìƒ‰ (ë„¤ì´ë²„ì¸ë° ë„ë©”ì¸ì´ ë¯¸ì„¸í•˜ê²Œ ë‹¤ë¥¸ ê²½ìš° ëŒ€ì‘)
            generic_selectors = ["#dic_area", "._article_content", "#articleBody"]
            for sel in generic_selectors:
                div = soup.select_one(sel)
                if div:
                    body = div.get_text(separator="\n", strip=True)
                    if len(body) > 100:
                         return body, driver
            
        # Selenium ì‹¤íŒ¨ ì‹œ Requests ì‹œë„
        return fallback_with_requests(url), driver

    except Exception as e:
        log(f"âš ï¸ í¬ë¡¤ë§ ì—ëŸ¬ â†’ Fallback ì‹œë„: {e}", index)
        return fallback_with_requests(url), driver

# [ë³€ê²½] excluded_domains ê´€ë ¨ í•¨ìˆ˜ ì‚­ì œë¨

MAX_QUERY_LENGTH = 100

def generate_search_queries(title, first, second, last, press, index=None):
    def truncate(text):
        return text[:MAX_QUERY_LENGTH] if text else ""
    
    title_clean = truncate(clean_text(title))
    first_clean = truncate(clean_text(first))
    
    keywords = truncate(extract_keywords(title_clean))
    if index is not None:
        log(f"ğŸ”‘ [í‚¤ì›Œë“œ] {title_clean[:15]}... -> {keywords}", index)

    queries = list(set(filter(None, [
        title_clean,
        f"{keywords} {press}" if press else keywords,
        first_clean,
    ])))
    
    if index is not None:
        log(f"ğŸ“œ [ê²€ìƒ‰ì–´ ëª©ë¡] {queries}", index)

    return queries

def extract_oid_from_naver_url(link):
    parsed = urlparse(link)
    path = parsed.path
    match = re.search(r"/article/(\d{3})/\d+", path)
    if match: return match.group(1)
    match = re.search(r"/mnews/article/(\d{3})/\d+", path)
    if match: return match.group(1)
    return None

def search_news_with_api(queries, driver, client_id, client_secret, max_results=3, index=None):
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret
    }
    results = []
    seen_links = set()

    log(f"ğŸš€ API ê²€ìƒ‰ ì§„ì… (ì¿¼ë¦¬ {len(queries)}ê°œ)", index)

    for i, q in enumerate(queries):
        log(f"   ğŸ” [{i+1}/{len(queries)}] ê²€ìƒ‰: '{q}'", index)
        
        url = f"https://openapi.naver.com/v1/search/news.json?query={urllib.parse.quote(q)}&display={max_results}&sort=sim"
        try:
            res = session.get(url, headers=headers, timeout=10)
            if res.status_code != 200:
                log(f"   âš ï¸ API ì‘ë‹µ ì‹¤íŒ¨: {res.status_code}", index)
                continue

            items = res.json().get("items", [])
            log(f"   âœ… ê²°ê³¼: {len(items)}ê±´ ë°œê²¬", index)

            for item in items:
                link = item.get("link")
                title = BeautifulSoup(item.get("title", ""), "html.parser").get_text()

                if not link or link in seen_links:
                    continue
                
                # [ë³€ê²½] ë„¤ì´ë²„ ê¸°ì‚¬ê°€ ì•„ë‹ˆë©´ ë¬´ì¡°ê±´ íŒ¨ìŠ¤ (ì œì™¸ ë„ë©”ì¸ ë¡œì§ ì‚­ì œ)
                if "naver.com" not in link:
                    # log(f"   â© ë„¤ì´ë²„ ê¸°ì‚¬ê°€ ì•„ë‹ˆë¼ íŒ¨ìŠ¤: {link}", index) 
                    continue

                if "naver.com" in link:
                    oid = extract_oid_from_naver_url(link)
                    if not oid: 
                        continue
                
                # ë³¸ë¬¸ ìˆ˜ì§‘
                body, new_driver = get_news_article_body(link, driver, index=index)
                if new_driver != driver:
                    driver = new_driver

                seen_links.add(link)
                if body and len(body) > 200:
                    cleaned_body = clean_text(body)
                    results.append({"title": title, "link": link, "body": cleaned_body})

        except Exception as e:
            log(f"   âŒ API ë£¨í”„ ì¤‘ ì—ëŸ¬: {e}", index)
            continue

    log(f"ğŸ ìœ íš¨ ê¸°ì‚¬ í™•ë³´: {len(results)}ê°œ", index)
    return results