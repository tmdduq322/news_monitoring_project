import os
import re
import time
import logging
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime

# ì•ˆì •ì ì¸ ë¡œë”©ì„ ìœ„í•œ Selenium ëª¨ë“ˆ
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# utils.pyì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ë“¤ ê°€ì ¸ì˜¤ê¸°
from .utils import setup_driver, save_to_csv, clean_title, result_csv_data

# [ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ í•¨ìˆ˜]
def fm_crw(wd, url, search, target_date):
    try:
        wd.get(url)
        
        # ë³¸ë¬¸ ì˜ì—­(div.xe_content)ì´ ëœ° ë•Œê¹Œì§€ ìµœëŒ€ 10ì´ˆ ëŒ€ê¸°
        try:
            WebDriverWait(wd, 10).until(
                EC.presence_of_element_located((By.CLASS_NAME, "xe_content"))
            )
        except:
            logging.error(f"âŒ í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ ë˜ëŠ” ì°¨ë‹¨ë¨: {url}")
            return

        soup = BeautifulSoup(wd.page_source, 'html.parser')

        # ë°ì´í„° ë‹´ì„ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        writer_list = []
        title_list = []
        content_list = []
        url_list = []
        search_plt_list = []
        search_word_list = []
        date_list = []
        now_date_list = []

        # 1. ì œëª© ì¶”ì¶œ (ì—…ë°ì´íŠ¸ëœ ì„ íƒì: span.np_18px_span)
        try:
            # h1 íƒœê·¸ ì•„ë˜ span.np_18px_span
            title_tag = soup.find('span', class_='np_18px_span')
            if not title_tag: # í˜¹ì‹œ ë‹¤ë¥¸ í´ë˜ìŠ¤ì¼ ê²½ìš° ëŒ€ë¹„
                 title_tag = soup.find('div', class_='top_area').find('h1')
            
            raw_title = title_tag.get_text()
            cleaned_title = clean_title(raw_title)
            title_list.append(cleaned_title)
        except Exception as e:
            logging.error(f"ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return 

        search_plt_list.append('ì›¹í˜ì´ì§€(ì—í¨ì½”ë¦¬ì•„)')
        url_list.append(url)

        # 2. ë³¸ë¬¸ ì¶”ì¶œ (ì—…ë°ì´íŠ¸ëœ ì„ íƒì: div.xe_content)
        try:
            content_div = soup.find('div', class_='xe_content')
            if content_div:
                post_content = content_div.get_text(separator=' ', strip=True)
                post_content_cleaned = re.sub(r'https?://[^\s]+', '', post_content).strip()
                content_list.append(post_content_cleaned)
            else:
                content_list.append('')
        except Exception:
            content_list.append('')

        search_word_list.append(search)

        # 3. ë‚ ì§œ ì¶”ì¶œ (ì—…ë°ì´íŠ¸ëœ ì„ íƒì: span.date.m_no)
        # ìƒì„¸í˜ì´ì§€ í˜•ì‹: 2026.01.19 12:59
        try:
            date_str = soup.find('span', class_='date m_no').get_text().strip()
            # YYYY.MM.DD -> YYYY-MM-DDë¡œ ë³€í™˜
            clean_date = date_str.split(' ')[0].replace('.', '-')
            date_list.append(clean_date)
        except:
            date_list.append(target_date)

        # 4. ì‘ì„±ì ì¶”ì¶œ (ì—…ë°ì´íŠ¸ëœ ì„ íƒì: a.member_plate)
        try:
            writer_tag = soup.find('a', class_='member_plate')
            writer_list.append(writer_tag.get_text(strip=True) if writer_tag else "Unknown")
        except:
            writer_list.append("Unknown")

        now_date_list.append(datetime.now().strftime('%Y-%m-%d'))

        # ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì €ì¥
        df = pd.DataFrame({
            "ê²€ìƒ‰ì–´": search_word_list,
            "í”Œë«í¼": search_plt_list,
            "ê²Œì‹œë¬¼ URL": url_list,
            "ê²Œì‹œë¬¼ ì œëª©": title_list,
            "ê²Œì‹œë¬¼ ë‚´ìš©": content_list,
            "ê²Œì‹œë¬¼ ë“±ë¡ì¼ì": date_list,
            "ê³„ì •ëª…": writer_list,
            "ìˆ˜ì§‘ì‹œê°„": now_date_list,
        })

        current_dir = os.path.dirname(__file__)
        save_path = os.path.join(current_dir, '..', 'data', 'raw', '23.ì—í¨ì½”ë¦¬ì•„', target_date)
        os.makedirs(save_path, exist_ok=True)
        file_name = os.path.join(save_path, f'ì—í¨ì½”ë¦¬ì•„_{search}.csv')
        
        save_to_csv(df, file_name)
        logging.info(f"âœ… ìˆ˜ì§‘ ì„±ê³µ: {cleaned_title[:15]}...")
        
    except Exception as e:
        logging.error(f"ìƒì„¸ í˜ì´ì§€ íŒŒì‹± ì˜¤ë¥˜: {e}")


# [ë©”ì¸ í¬ë¡¤ë§ ì§„ì…ì ]
def fm_main_crw(searchs, start_date, end_date, stop_event):
    target_date = start_date.strftime("%y%m%d")
    
    # ë¡œê¹… ì„¤ì •
    current_dir = os.path.dirname(__file__)
    project_root = os.path.abspath(os.path.join(current_dir, '..'))
    log_dir = os.path.join(project_root, 'log')
    os.makedirs(log_dir, exist_ok=True)
    
    logging.basicConfig(
        filename=os.path.join(log_dir, f'ì—í¨ì½”ë¦¬ì•„_log_{target_date}.txt'),
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        encoding='utf-8',
        force=True 
    )

    logging.info(f"ğŸš€ ì—í¨ì½”ë¦¬ì•„ í¬ë¡¤ë§ ì‹œì‘ (Date: {target_date})")
    
    wd = setup_driver()     # ëª©ë¡ íƒìƒ‰ìš©
    wd_dp1 = setup_driver() # ìƒì„¸ í˜ì´ì§€ìš©

    for search in searchs:
        if stop_event.is_set():
            break
            
        page_num = 1
        while True:
            try:
                # https://m.kpedia.jp/w/7006 í†µí•©ê²€ìƒ‰(ë¬¸ì„œ íƒ­) URL êµ¬ì¡° (where=document í•„ìˆ˜)
                url_list_page = (
                    f"https://www.fmkorea.com/index.php?act=IS&is_keyword={search}"
                    f"&mid=home&where=document&page={page_num}"
                )
                
                wd.get(url_list_page)
                
                # ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸(ul.searchResult) ë¡œë”© ëŒ€ê¸°
                try:
                    WebDriverWait(wd, 10).until(
                        EC.presence_of_element_located((By.CLASS_NAME, 'searchResult'))
                    )
                except:
                    logging.info("ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ ë˜ëŠ” í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨")
                    break

                soup = BeautifulSoup(wd.page_source, 'html.parser')
                
                # [ì¤‘ìš”] ê²Œì‹œê¸€ ëª©ë¡(ul.searchResult) ê°€ì ¸ì˜¤ê¸°
                # ì£¼ì˜: 'comment' í´ë˜ìŠ¤ê°€ í¬í•¨ëœ ulì€ ëŒ“ê¸€ ê²€ìƒ‰ ê²°ê³¼ì´ë¯€ë¡œ ì œì™¸í•´ì•¼ í•¨
                result_uls = soup.find_all('ul', class_='searchResult')
                target_ul = None
                
                for ul in result_uls:
                    # class ë¦¬ìŠ¤íŠ¸ì— 'comment'ê°€ ì—†ìœ¼ë©´ ìš°ë¦¬ê°€ ì°¾ëŠ” ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì„
                    if 'comment' not in ul.get('class', []):
                        target_ul = ul
                        break
                
                if not target_ul:
                    logging.info("ê²Œì‹œê¸€ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ëŒ“ê¸€ ëª©ë¡ë§Œ ìˆê±°ë‚˜ ê²°ê³¼ ì—†ìŒ)")
                    break
                    
                li_tags = target_ul.find_all('li')
                if not li_tags:
                    break

                stop_crawling = False

                for li in li_tags:
                    try:
                        # [ë‚ ì§œ ì¶”ì¶œ] ëª©ë¡ì˜ ë‚ ì§œ: 2026-01-19 12:59 (í˜•ì‹: YYYY-MM-DD HH:MM)
                        time_span = li.find('span', class_='time')
                        if not time_span:
                            continue
                            
                        date_str = time_span.get_text().strip()
                        post_date_str = date_str.split(' ')[0] 
                        post_date = datetime.strptime(post_date_str, '%Y-%m-%d').date()

                        # ë‚ ì§œ í•„í„°ë§
                        if post_date > end_date:
                            continue 
                        if post_date < start_date:
                            stop_crawling = True
                            break

                        # [ë§í¬ ë° ì œëª© ì¶”ì¶œ] dt > a href
                        dt_tag = li.find('dt')
                        if dt_tag and dt_tag.find('a'):
                            a_tag = dt_tag.find('a')
                            link_part = a_tag['href']
                            
                            # ë§í¬ê°€ /939... í˜•íƒœë¼ë©´ ì•ì— ë„ë©”ì¸ ë¶™ì´ê¸°
                            if link_part.startswith('/'):
                                full_url = f"https://www.fmkorea.com{link_part}"
                            else:
                                full_url = link_part
                            
                            logging.info(f"url ì°¾ìŒ: {full_url}")
                            fm_crw(wd_dp1, full_url, search, target_date)
                            
                    except Exception as e:
                        logging.error(f"ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ íŒŒì‹± ì—ëŸ¬: {e}")
                        continue

                if stop_crawling:
                    logging.info(f"ì„¤ì • ê¸°ê°„({start_date}) ì´ì „ ë°ì´í„° ë„ë‹¬. ë‹¤ìŒ ê²€ìƒ‰ì–´ë¡œ ì´ë™.")
                    break
                
                page_num += 1
                time.sleep(1) # ê³¼ë¶€í•˜ ë°©ì§€

            except Exception as e:
                logging.error(f"í˜ì´ì§€ ìˆœíšŒ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
                break

    wd.quit()
    wd_dp1.quit()
    
    # ê²°ê³¼ ë³‘í•©
    result_dir = os.path.join(project_root, 'data', 'raw')
    try:
        all_data = pd.concat([
            result_csv_data(search, platform='ì—í¨ì½”ë¦¬ì•„', subdir=f'23.ì—í¨ì½”ë¦¬ì•„/{target_date}', base_path='data/raw')
            for search in searchs
        ])
        all_data.to_csv(os.path.join(result_dir, f'ì—í¨ì½”ë¦¬ì•„_raw_{target_date}.csv'), encoding='utf-8', index=False)
        logging.info(f"ìµœì¢… ë³‘í•© ì™„ë£Œ: ì—í¨ì½”ë¦¬ì•„_raw_{target_date}.csv")
    except ValueError:
        logging.warning("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")