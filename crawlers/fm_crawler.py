import os
import re
import time
import random
import logging
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime

# [ì¶”ê°€] ì•ˆì •ì ì¸ ë¡œë”©ì„ ìœ„í•œ Selenium ëŒ€ê¸° ëª¨ë“ˆ
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from .utils import setup_driver, save_to_csv, clean_title, result_csv_data

# í•œí˜ì´ì§€ í¬ë¡¤ë§ (ìƒì„¸ í˜ì´ì§€)
def fm_crw(wd, url, search, target_date):
    try:
        wd.get(url)
        # [ê°œì„ ] ë³¸ë¬¸(article)ì´ ë¡œë”©ë  ë•Œê¹Œì§€ ìµœëŒ€ 10ì´ˆ ëŒ€ê¸°
        try:
            WebDriverWait(wd, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "article"))
            )
        except:
            logging.error(f"âŒ í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ ë˜ëŠ” ì°¨ë‹¨ë¨: {url}")
            return

        soup = BeautifulSoup(wd.page_source, 'html.parser')

        # 1. ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸° (ì œê³µëœ HTML êµ¬ì¡° ë°˜ì˜)
        content_div = soup.find('article')
        if not content_div:
            logging.error(f"âŒ ë³¸ë¬¸(article) ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {url}")
            return

        # 2. ì œëª© ì¶”ì¶œ (np_18px_span í´ë˜ìŠ¤ ì‚¬ìš©)
        title_tag = soup.find('span', class_='np_18px_span')
        raw_title = title_tag.get_text() if title_tag else "ì œëª© ì—†ìŒ"
        cleaned_title = clean_title(raw_title)
        logging.info(f"ì œëª© ì¶”ì¶œ ì„±ê³µ: {cleaned_title}")

        # 3. ë³¸ë¬¸ ë‚´ ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° (<a> íƒœê·¸ ë‚´ ë¯¸ë””ì–´ ì—†ëŠ” ê²½ìš°)
        a_tags = content_div.find_all('a')
        for a_tag in a_tags:
            if (
                    not a_tag.find('img') and
                    not a_tag.find('span', class_='scrap_img') and
                    not a_tag.find('video') and
                    not (a_tag.find('iframe') and 'youtube.com' in a_tag.decode_contents())
            ):
                a_tag.decompose()

        post_content = content_div.get_text(separator='\n', strip=True)
        post_content = re.sub(r'http[s]?://\S+', '', post_content)
        logging.info(f"ë‚´ìš© ì¶”ì¶œ ì„±ê³µ (ê¸¸ì´: {len(post_content)})")

        # 4. ë‚ ì§œ ë° ì‘ì„±ì ì¶”ì¶œ (ì œê³µëœ HTML êµ¬ì¡° ë°˜ì˜)
        # ë‚ ì§œ: <span class="date m_no">2026.01.17 23:30</span>
        date_tag = soup.find('span', class_="date m_no")
        date_val = date_tag.text.split()[0].replace('.', '-') if date_tag else target_date

        # ì‘ì„±ì: <a class="member_...">
        writer_tag = soup.find('a', class_=re.compile(r'^member_\d+'))
        writer_val = writer_tag.get_text() if writer_tag else "ìµëª…"

        now_time = datetime.now().strftime('%Y-%m-%d ')
           
        main_temp = pd.DataFrame({
            "ê²€ìƒ‰ì–´": [search],
            "í”Œë«í¼": ['ì›¹í˜ì´ì§€(ì—í¨ì½”ë¦¬ì•„)'],
            "ê²Œì‹œë¬¼ URL": [url],
            "ê²Œì‹œë¬¼ ì œëª©": [cleaned_title],
            "ê²Œì‹œë¬¼ ë‚´ìš©": [post_content],
            "ê²Œì‹œë¬¼ ë“±ë¡ì¼ì": [date_val],
            "ê³„ì •ëª…": [writer_val],
            "ìˆ˜ì§‘ì‹œê°„": [now_time],
        })

        # 5. ì €ì¥ ê²½ë¡œ ì„¤ì • ë° ì €ì¥
        current_dir = os.path.dirname(__file__)
        save_path = os.path.join(current_dir, '..', 'data', 'raw', '23.ì—í¨ì½”ë¦¬ì•„', target_date)
        os.makedirs(save_path, exist_ok=True)
        
        file_name = os.path.join(save_path, f'ì—í¨ì½”ë¦¬ì•„_{search}.csv')
        save_to_csv(main_temp, file_name)
        logging.info(f'âœ… ì €ì¥ì™„ë£Œ : {file_name}')

    except Exception as e:
        logging.error(f"ğŸ›‘ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


def fm_main_crw(searchs, start_date, end_date, stop_event):
    target_date = start_date.strftime("%y%m%d")
    current_dir = os.path.dirname(__file__)
    project_root = os.path.abspath(os.path.join(current_dir, '..'))
    
    # ë¡œê·¸ ì„¤ì •
    log_dir = os.path.join(project_root, 'log')
    os.makedirs(log_dir, exist_ok=True)
    logging.basicConfig(
        filename=os.path.join(log_dir, f'ì—í¨ì½”ë¦¬ì•„_log_{target_date}.txt'),
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        encoding='utf-8',
        force=True
    )

    logging.info(f"ğŸš€ ì—í¨ì½”ë¦¬ì•„ í¬ë¡¤ë§ ì‹œì‘ (Date: {target_date})")
    
    wd = setup_driver()
    wd_dp1 = setup_driver()

    for search in searchs:
        if stop_event.is_set():
            break
        page_num = 1

        while True:
            if stop_event.is_set():
                break
            try:
                url_dp1 = f'https://www.fmkorea.com/search.php?act=IS&is_keyword={search}&mid=home&where=document&page={page_num}'
                wd_dp1.get(url_dp1)
                time.sleep(random.uniform(2, 4))
                soup_dp1 = BeautifulSoup(wd_dp1.page_source, 'html.parser')

                # [ìˆ˜ì •] ê²€ìƒ‰ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ì˜ì—­ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ì—ëŸ¬ ë°©ì§€)
                search_result_ul = soup_dp1.find('ul', class_='searchResult')
                if not search_result_ul:
                    logging.info(f"ê²€ìƒ‰ ê²°ê³¼ê°€ ë” ì´ìƒ ì—†ê±°ë‚˜ ì°¨ë‹¨ë¨ (Page: {page_num})")
                    break

                li_tags = search_result_ul.find_all('li')
                after_start_date = False

                for li in li_tags:
                    try:
                        # ë¦¬ìŠ¤íŠ¸ì—ì„œì˜ ë‚ ì§œ ì¶”ì¶œ (êµ¬ì¡° í™•ì¸ í•„ìš”, ì¼ë°˜ì ìœ¼ë¡œ span.time í˜¹ì€ span.date ì‚¬ìš©)
                        date_tag = li.find('span', class_=re.compile(r'time|date'))
                        if not date_tag: continue
                        
                        date_str = date_tag.text.strip()
                        # ë¦¬ìŠ¤íŠ¸ ë‚ ì§œ í˜•ì‹ì— ë”°ë¥¸ íŒŒì‹± (YYYY-MM-DD HH:MM ë˜ëŠ” YYYY.MM.DD)
                        if '.' in date_str:
                            item_date = datetime.strptime(date_str.split()[0], '%Y.%m.%d').date()
                        else:
                            item_date = datetime.strptime(date_str.split()[0], '%Y-%m-%d').date()
                    except Exception as e:
                        continue

                    if item_date > end_date:
                        continue
                    if item_date < start_date:
                        after_start_date = True
                        break

                    a_tag = li.find('a')
                    if a_tag:
                        url = 'https://www.fmkorea.com' + a_tag.get('href')
                        fm_crw(wd, url, search, target_date)

                if after_start_date:
                    break
                else:
                    page_num += 1

            except Exception as e:
                logging.error(f"âš ï¸ ê²€ìƒ‰ ë¦¬ìŠ¤íŠ¸ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                break
                
    wd.quit()
    wd_dp1.quit()
    
    if not stop_event.is_set():
        result_dir = os.path.join(project_root, 'ê²°ê³¼', 'ì—í¨ì½”ë¦¬ì•„')
        os.makedirs(result_dir, exist_ok=True)

        all_data = pd.concat([
            result_csv_data(search, platform='ì—í¨ì½”ë¦¬ì•„', subdir=f'23.ì—í¨ì½”ë¦¬ì•„/{target_date}', base_path='data/raw')
            for search in searchs
        ])
        all_data.to_csv(os.path.join(result_dir, f'ì—í¨ì½”ë¦¬ì•„_raw data_{target_date}.csv'), encoding='utf-8', index=False)