import os
import re
import time
import logging
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime

# ì•ˆì •ì ì¸ ë¡œë”©ì„ ìœ„í•œ Selenium ëŒ€ê¸° ëª¨ë“ˆ
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from .utils import setup_driver, save_to_csv, clean_title, result_csv_data

# [ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§]
def fm_crw(wd, url, search, target_date):
    try:
        wd.get(url)
        # ë³¸ë¬¸ ì˜ì—­(xe_content)ì´ ëœ° ë•Œê¹Œì§€ ìµœëŒ€ 10ì´ˆ ëŒ€ê¸°
        try:
            WebDriverWait(wd, 10).until(
                EC.presence_of_element_located((By.CLASS_NAME, "xe_content"))
            )
        except:
            logging.error(f"âŒ í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ ë˜ëŠ” ì°¨ë‹¨ë¨: {url}")
            return pd.DataFrame()

        soup = BeautifulSoup(wd.page_source, 'html.parser')

        # ë°ì´í„° ë‹´ì„ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        writer_list = []
        title_list = []
        content_list = []
        url_list = []
        search_plt_list = []
        search_word_list = []
        date_list = []
        now_date_list = []

        # 1. ì œëª© ì¶”ì¶œ (ì—…ë°ì´íŠ¸ëœ ì„ íƒì: span.np_18px_span)
        try:
            # h1 íƒœê·¸ ì•„ë˜ span.np_18px_span
            raw_title = soup.find('span', class_='np_18px_span').get_text()
            cleaned_title = clean_title(raw_title)
            title_list.append(cleaned_title)
        except Exception as e:
            logging.error(f"ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

        search_plt_list.append('ì›¹í˜ì´ì§€(ì—í¨ì½”ë¦¬ì•„)')
        url_list.append(url)

        # 2. ë³¸ë¬¸ ì¶”ì¶œ (ì—…ë°ì´íŠ¸ëœ ì„ íƒì: div.xe_content)
        try:
            content_div = soup.find('div', class_='xe_content')
            if content_div:
                post_content = content_div.get_text(separator=' ', strip=True)
                # URL ë“± ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
                post_content_cleaned = re.sub(r'https?://[^\s]+', '', post_content).strip()
                content_list.append(post_content_cleaned)
            else:
                content_list.append('')
        except Exception:
            content_list.append('')

        search_word_list.append(search)

        # 3. ë‚ ì§œ ì¶”ì¶œ (ì—…ë°ì´íŠ¸ëœ ì„ íƒì: span.date.m_no)
        # í˜•ì‹: 2026.01.19 12:59
        try:
            date_str = soup.find('span', class_='date m_no').get_text().strip()
            # YYYY.MM.DD HH:MM -> YYYY-MM-DDë¡œ ë³€í™˜
            clean_date = date_str.split(' ')[0].replace('.', '-')
            date_list.append(clean_date)
        except:
            # ì‹¤íŒ¨ ì‹œ íƒ€ê²Ÿ ë‚ ì§œ(ì˜¤ëŠ˜/ì–´ì œ)ë¡œ ëŒ€ì²´í•˜ê±°ë‚˜ ë¹ˆì¹¸
            date_list.append(target_date)

        # 4. ì‘ì„±ì ì¶”ì¶œ (ì—…ë°ì´íŠ¸ëœ ì„ íƒì: a.member_plate)
        try:
            writer_tag = soup.find('a', class_='member_plate')
            writer_list.append(writer_tag.get_text(strip=True) if writer_tag else "Unknown")
        except:
            writer_list.append("Unknown")

        now_date_list.append(datetime.now().strftime('%Y-%m-%d'))

        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        df = pd.DataFrame({
            "ê²€ìƒ‰ì–´": search_word_list,
            "í”Œë«í¼": search_plt_list,
            "ê²Œì‹œë¬¼ URL": url_list,
            "ê²Œì‹œë¬¼ ì œëª©": title_list,
            "ê²Œì‹œë¬¼ ë‚´ìš©": content_list,
            "ê²Œì‹œë¬¼ ë“±ë¡ì¼ì": date_list,
            "ê³„ì •ëª…": writer_list,
            "ìˆ˜ì§‘ì‹œê°„": now_date_list,
        })

        # ì €ì¥ ë¡œì§
        current_dir = os.path.dirname(__file__)
        save_path = os.path.join(current_dir, '..', 'data', 'raw', '23.ì—í¨ì½”ë¦¬ì•„', target_date)
        os.makedirs(save_path, exist_ok=True)
        file_name = os.path.join(save_path, f'ì—í¨ì½”ë¦¬ì•„_{search}.csv')
        
        save_to_csv(df, file_name)
        logging.info(f"âœ… ìˆ˜ì§‘ ì„±ê³µ: {cleaned_title[:15]}...")
        
    except Exception as e:
        logging.error(f"ìƒì„¸ í˜ì´ì§€ íŒŒì‹± ì˜¤ë¥˜: {e}")

# [ë©”ì¸ í¬ë¡¤ë§ ì§„ì…ì ]
def fm_main_crw(searchs, start_date, end_date, stop_event):
    target_date = start_date.strftime("%y%m%d")
    
    # ë¡œê¹… ì„¤ì •
    current_dir = os.path.dirname(__file__)
    project_root = os.path.abspath(os.path.join(current_dir, '..'))
    log_dir = os.path.join(project_root, 'log')
    os.makedirs(log_dir, exist_ok=True)
    
    logging.basicConfig(
        filename=os.path.join(log_dir, f'ì—í¨ì½”ë¦¬ì•„_log_{target_date}.txt'),
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        encoding='utf-8',
        force=True 
    )

    logging.info(f"ğŸš€ ì—í¨ì½”ë¦¬ì•„ í¬ë¡¤ë§ ì‹œì‘ (Date: {target_date})")
    
    wd = setup_driver()    # ëª©ë¡ íƒìƒ‰ìš©
    wd_dp1 = setup_driver() # ìƒì„¸ í˜ì´ì§€ìš© (ì¶©ëŒ ë°©ì§€ ìœ„í•´ ë¶„ë¦¬ ì¶”ì²œ)

    for search in searchs:
        if stop_event.is_set():
            break
            
        page_num = 1
        while True:
            try:
                # https://www.youtube.com/watch?v=pSE0tdUwW58 í†µí•©ê²€ìƒ‰(ë¬¸ì„œ íƒ­) URL êµ¬ì¡°
                # https://www.fmkorea.com/index.php?act=IS&is_keyword={ê²€ìƒ‰ì–´}&mid=home&where=document&page={í˜ì´ì§€}
                url_list_page = (
                    f"https://www.fmkorea.com/index.php?act=IS&is_keyword={search}"
                    f"&mid=home&where=document&page={page_num}"
                )
                
                wd.get(url_list_page)
                
                # ê²€ìƒ‰ ê²°ê³¼(ul.searchResult) ë¡œë”© ëŒ€ê¸°
                try:
                    WebDriverWait(wd, 10).until(
                        EC.presence_of_element_located((By.CLASS_NAME, 'searchResult'))
                    )
                except:
                    logging.info("ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ ë˜ëŠ” í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨")
                    break

                soup = BeautifulSoup(wd.page_source, 'html.parser')
                
                # ê²Œì‹œê¸€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                result_ul = soup.find('ul', class_='searchResult')
                if not result_ul:
                    break
                    
                li_tags = result_ul.find_all('li')
                if not li_tags:
                    break

                # ë‚ ì§œ ì²´í¬ë¥¼ ìœ„í•œ í”Œë˜ê·¸
                stop_crawling = False

                for li in li_tags:
                    try:
                        # [ë‚ ì§œ ì¶”ì¶œ] ëª©ë¡ì˜ ë‚ ì§œ: 2026-01-19 12:59 (í˜•ì‹: YYYY-MM-DD HH:MM)
                        time_span = li.find('span', class_='time')
                        if not time_span:
                            continue
                            
                        date_str = time_span.get_text().strip()
                        # ì‹œê°„ ë¶€ë¶„ ì œê±°í•˜ê³  ë‚ ì§œë§Œ ë¹„êµ
                        post_date_str = date_str.split(' ')[0] 
                        post_date = datetime.strptime(post_date_str, '%Y-%m-%d').date()

                        # ë‚ ì§œ í•„í„°ë§
                        if post_date > end_date:
                            continue # ë¯¸ë˜ ë‚ ì§œ(ë˜ëŠ” ë²”ìœ„ ë°–)ëŠ” íŒ¨ìŠ¤
                        if post_date < start_date:
                            stop_crawling = True
                            break

                        # [ë§í¬ ì¶”ì¶œ] dt > a href
                        dt_tag = li.find('dt')
                        if dt_tag and dt_tag.find('a'):
                            link_part = dt_tag.find('a')['href']
                            # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜ (/939... -> https://www.fmkorea.com/939...)
                            full_url = f"https://www.fmkorea.com{link_part}"
                            
                            logging.info(f"url ì°¾ìŒ: {full_url}")
                            fm_crw(wd_dp1, full_url, search, target_date)
                            
                    except Exception as e:
                        logging.error(f"ë¦¬ìŠ¤íŠ¸ íŒŒì‹± ì¤‘ ì—ëŸ¬: {e}")
                        continue

                if stop_crawling:
                    logging.info(f"ì„¤ì • ê¸°ê°„({start_date}) ì´ì „ ë°ì´í„° ë„ë‹¬. ë‹¤ìŒ ê²€ìƒ‰ì–´ë¡œ ì´ë™.")
                    break
                
                page_num += 1
                # ë„ˆë¬´ ë¹ ë¥¸ ìš”ì²­ ë°©ì§€
                time.sleep(1)

            except Exception as e:
                logging.error(f"í˜ì´ì§€ ìˆœíšŒ ì¤‘ ì˜¤ë¥˜: {e}")
                break

    wd.quit()
    wd_dp1.quit()
    
    # ê²°ê³¼ ë³‘í•©
    result_dir = os.path.join(project_root, 'data', 'raw')
    try:
        all_data = pd.concat([
            result_csv_data(search, platform='ì—í¨ì½”ë¦¬ì•„', subdir=f'23.ì—í¨ì½”ë¦¬ì•„/{target_date}', base_path='data/raw')
            for search in searchs
        ])
        all_data.to_csv(os.path.join(result_dir, f'ì—í¨ì½”ë¦¬ì•„_raw_{target_date}.csv'), encoding='utf-8', index=False)
    except ValueError:
        logging.warning("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")