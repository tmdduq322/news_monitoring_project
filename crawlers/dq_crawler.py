import os
import re
import time
import random
import logging
import pandas as pd
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from selenium.common.exceptions import WebDriverException
from datetime import datetime,date

from .utils import setup_driver, save_to_csv, clean_title,result_csv_data

# ì‹¤í–‰ë‚ ì§œ ë³€ìˆ˜ ë° í´ë” ìƒì„±
today = datetime.now().strftime("%y%m%d")
if not os.path.exists(f'log'):
    os.makedirs(f'log')

logging.basicConfig(
    filename=f'ë”ì¿ _log_{today}.txt',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    encoding='utf-8'
)
today = datetime.now().strftime("%y%m%d")
def parse_theqoo_date(raw_text):
    today = date.today()
    try:
        # 1. "HH:MM" â†’ ì˜¤ëŠ˜ ë‚ ì§œ
        if re.match(r"^\d{2}:\d{2}$", raw_text):
            return today

        # 2. "MM.DD" â†’ ì˜¬í•´ ë‚ ì§œ
        elif re.match(r"^\d{2}\.\d{2}$", raw_text):
            month, day = map(int, raw_text.split('.'))
            return date(today.year, month, day)

        # 3. "YY.MM.DD" â†’ ì—°ë„ í¬í•¨
        elif re.match(r"^\d{2}\.\d{2}\.\d{2}$", raw_text):
            return datetime.strptime(raw_text, "%y.%m.%d").date()

        else:
            logging.warning(f"ë‚ ì§œ í˜•ì‹ ì¸ì‹ ë¶ˆê°€: {raw_text}")
            return None
    except Exception as e:
        logging.error(f"ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {raw_text}, ì˜¤ë¥˜: {e}")
        return None

def dq_crw(wd, url, searchs):
    try:
        logging.info(f"ë”ì¿  í¬ë¡¤ë§ ì‹œì‘: {url}")
        wd.get(url)
        time.sleep(2)
        WebDriverWait(wd, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.theqoo_document_header > span.title"))
        )

        soup = BeautifulSoup(wd.page_source, 'html.parser')

        raw_title = wd.find_element(By.CSS_SELECTOR, "div.theqoo_document_header > span.title").text.strip()
        cleaned_title = clean_title(raw_title)

        article_tag = soup.find('article', attrs={"itemprop": "articleBody"})
        if not article_tag:
            print("ë³¸ë¬¸ <article> íƒœê·¸ê°€ ì—†ìŒ")
            return

        content_div = article_tag.find('div', class_=lambda x: x and 'rhymix_content' in x and 'xe_content' in x)
        if not content_div:
            print("ë³¸ë¬¸ ë‚´ìš© divë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return

        post_content = wd.find_element(By.CSS_SELECTOR, "article[itemprop='articleBody']").text
        post_content = re.sub(r'http[s]?://\S+', '', post_content).strip()

        # ë‚ ì§œ íŒŒì‹±
        date_tag = soup.select_one('div.side.fr > span')
        if not date_tag:
            logging.warning(f"ë‚ ì§œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤: {url}")
            return
        date_str = date_tag.get_text(strip=True)
        try:
            date = datetime.strptime(date_str.split()[0], '%Y.%m.%d').date()
        except Exception as e:
            logging.error(f"ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {date_str}, ì˜¤ë¥˜: {e}")
            return

        writer_tag = soup.select_one('div.side')
        if writer_tag:
            # ë§í¬ ì œì™¸í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
            writer = ''.join([t for t in writer_tag.contents if isinstance(t, str)]).strip()
            if writer == "ë¬´ëª…ì˜ ë”ì¿ ":
                writer = "ìµëª…"
        else:
            writer = "ìµëª…"
        # images = content_div.find_all('img')
        # videos = content_div.find_all('video')
        # youtube_iframes = [i for i in content_div.find_all('iframe') if 'youtube.com' in str(i.get('src'))]
        # has_media = 'O' if images or videos or youtube_iframes else ' '

        now_time = datetime.now().strftime('%Y-%m-%d ')

        #  ë§¤ì¹­ë˜ëŠ” ê²€ìƒ‰ì–´ê°€ ìˆì„ ë•Œë§ˆë‹¤ ì €ì¥
        for search in searchs:
            if search.lower() in cleaned_title.lower() or search.lower() in post_content.lower():
                df = pd.DataFrame({
                    "ê²€ìƒ‰ì–´": [search],
                    "í”Œë«í¼": ["ì›¹í˜ì´ì§€(ë”ì¿ )"],
                    "ê²Œì‹œë¬¼ URL": [url],
                    "ê²Œì‹œë¬¼ ì œëª©": [cleaned_title],
                    "ê²Œì‹œë¬¼ ë‚´ìš©": [post_content],
                    "ê²Œì‹œë¬¼ ë“±ë¡ì¼ì": [date],
                    "ê³„ì •ëª…": [writer],
                    "ìˆ˜ì§‘ì‹œê°„": [now_time],
                    # "ì´ë¯¸ì§€ ìœ ë¬´": [has_media]
                })
                save_to_csv(df, f'data/raw/24.ë”ì¿ /{today}/ë”ì¿ _{search}.csv')
                logging.info(f'data/raw/24.ë”ì¿ /{today}/ì¸ìŠ¤í‹°ì¦ˆ_{search}.csv')

    except Exception as e:
        logging.error(f"ìƒì„¸ í˜ì´ì§€ ì˜¤ë¥˜: {e}")
        print(f"ìƒì„¸ í˜ì´ì§€ ì˜¤ë¥˜: {e}")


def dq_main_crw(searchs, start_date, end_date,stop_event, max_pages=1400):
    if not os.path.exists(f'data/raw/24.ë”ì¿ /{today}'):
        os.makedirs(f'data/raw/24.ë”ì¿ /{today}')
        print(f"í´ë” ìƒì„± ì™„ë£Œ: {today}")
    else:
        print(f"í•´ë‹¹ í´ë” ì¡´ì¬")
    logging.info(f"========================================================")
    logging.info(f"                    ë”ì¿  í¬ë¡¤ë§ ì‹œì‘")
    logging.info(f"========================================================")
    wd = setup_driver()
    wd_detail = setup_driver()

    page_num = 50
    visited_urls = set()

    while page_num <= max_pages:
        if stop_event.is_set():
            print("ğŸ›‘ í¬ë¡¤ë§ ì¤‘ë‹¨ë¨")
            break
        url_list_page = f'https://theqoo.net/square/category/512000849?page={page_num}'
        logging.info(f"[{page_num}í˜ì´ì§€] ì ‘ì†: {url_list_page}")
        print(f"[{page_num}í˜ì´ì§€] ì ‘ê·¼ ì¤‘...")

        try:
            wd.get(url_list_page)
            time.sleep(random.uniform(2, 3))

            WebDriverWait(wd, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, 'tbody.hide_notice tr'))
            )

            soup = BeautifulSoup(wd.page_source, 'html.parser')
            post_list = soup.select('tbody.hide_notice tr:not(.notice)')

            all_old = True
            stop_flag = False

            for post in post_list:
                if stop_event.is_set():
                    break
                try:
                    date_str = post.select_one('.time').get_text(strip=True)
                    post_date = parse_theqoo_date(date_str)
                    if not post_date:
                        continue

                    if post_date >= start_date:
                        all_old = False
                    if post_date > end_date:
                        continue
                    if post_date < start_date:
                        stop_flag = True
                        continue

                    title_tag = post.select_one('td.title > a:not(.replyNum)')
                    if not title_tag:
                        continue

                    post_url = 'https://theqoo.net' + title_tag.get('href')
                    if post_url in visited_urls:
                        continue
                    visited_urls.add(post_url)

                    #  ê²€ìƒ‰ì–´ ë¦¬ìŠ¤íŠ¸ í†µì§¸ë¡œ ì „ë‹¬
                    dq_crw(wd_detail, post_url, searchs)

                except Exception as e:
                    logging.error(f"ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue

            if all_old:
                page_num += 1
                continue

            if stop_flag:
                break

            page_num += 1

        except WebDriverException as e:
            logging.error(f"{page_num}í˜ì´ì§€ WebDriver ì˜ˆì™¸ ë°œìƒ: {e}")
            print(f"âŒ WebDriver ì˜ˆì™¸ ë°œìƒ! ë“œë¼ì´ë²„ ì¬ì‹œì‘ ì¤‘...")
            wd.quit()
            wd = setup_driver()
            page_num += 1
            continue

    wd.quit()
    wd_detail.quit()

    if not stop_event.is_set():
        result_dir = 'ê²°ê³¼/ë”ì¿ '
        if not os.path.exists(result_dir):
            os.makedirs(result_dir)

        all_data = pd.concat([
            result_csv_data(search, platform='ë”ì¿ ', subdir='24.ë”ì¿ ')
            for search in searchs
        ])

        all_data.to_csv(f'{result_dir}/ë”ì¿ _raw data_{today}.csv', encoding='utf-8', index=False)


