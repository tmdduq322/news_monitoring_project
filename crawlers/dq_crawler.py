import os
import re
import time
import random
import logging
import pandas as pd
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from selenium.common.exceptions import WebDriverException
from datetime import datetime, date

from .utils import setup_driver, save_to_csv, clean_title, result_csv_data

def parse_theqoo_date(raw_text):
    today = date.today()
    try:
        # 1. "HH:MM" â†’ ì˜¤ëŠ˜ ë‚ ì§œ
        if re.match(r"^\d{2}:\d{2}$", raw_text):
            return today
        # 2. "MM.DD" â†’ ì˜¬í•´ ë‚ ì§œ
        elif re.match(r"^\d{2}\.\d{2}$", raw_text):
            month, day = map(int, raw_text.split('.'))
            return date(today.year, month, day)
        # 3. "YY.MM.DD" â†’ ì—°ë„ í¬í•¨
        elif re.match(r"^\d{2}\.\d{2}\.\d{2}$", raw_text):
            return datetime.strptime(raw_text, "%y.%m.%d").date()
        else:
            logging.warning(f"ë‚ ì§œ í˜•ì‹ ì¸ì‹ ë¶ˆê°€: {raw_text}")
            return None
    except Exception as e:
        logging.error(f"ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {raw_text}, ì˜¤ë¥˜: {e}")
        return None

def dq_crw(wd, url, searchs, target_date):
    try:
        logging.info(f"ë”ì¿  í¬ë¡¤ë§ ì‹œì‘: {url}")
        wd.get(url)
        time.sleep(2)
        WebDriverWait(wd, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.theqoo_document_header > span.title"))
        )

        soup = BeautifulSoup(wd.page_source, 'html.parser')

        raw_title = wd.find_element(By.CSS_SELECTOR, "div.theqoo_document_header > span.title").text.strip()
        cleaned_title = clean_title(raw_title)

        article_tag = soup.find('article', attrs={"itemprop": "articleBody"})
        if not article_tag:
            print("ë³¸ë¬¸ <article> íƒœê·¸ê°€ ì—†ìŒ")
            return

        content_div = article_tag.find('div', class_=lambda x: x and 'rhymix_content' in x and 'xe_content' in x)
        if not content_div:
            print("ë³¸ë¬¸ ë‚´ìš© divë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return

        post_content = wd.find_element(By.CSS_SELECTOR, "article[itemprop='articleBody']").text
        post_content = re.sub(r'http[s]?://\S+', '', post_content).strip()

        # ë‚ ì§œ íŒŒì‹±
        date_tag = soup.select_one('div.side.fr > span')
        if not date_tag:
            logging.warning(f"ë‚ ì§œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤: {url}")
            return
        date_str = date_tag.get_text(strip=True)
        try:
            date = datetime.strptime(date_str.split()[0], '%Y.%m.%d').date()
        except Exception as e:
            logging.error(f"ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {date_str}, ì˜¤ë¥˜: {e}")
            return

        writer_tag = soup.select_one('div.side')
        if writer_tag:
            writer = ''.join([t for t in writer_tag.contents if isinstance(t, str)]).strip()
            if writer == "ë¬´ëª…ì˜ ë”ì¿ ":
                writer = "ìµëª…"
        else:
            writer = "ìµëª…"

        now_time = datetime.now().strftime('%Y-%m-%d ')

        # [ìˆ˜ì •] ì ˆëŒ€ ê²½ë¡œ ì €ì¥ ì„¤ì •
        current_dir = os.path.dirname(__file__)
        save_path = os.path.join(current_dir, '..', 'data', 'raw', '24.ë”ì¿ ', target_date)
        os.makedirs(save_path, exist_ok=True)

        # ë§¤ì¹­ë˜ëŠ” ê²€ìƒ‰ì–´ê°€ ìˆì„ ë•Œë§ˆë‹¤ ì €ì¥
        for search in searchs:
            if search.lower() in cleaned_title.lower() or search.lower() in post_content.lower():
                df = pd.DataFrame({
                    "ê²€ìƒ‰ì–´": [search],
                    "í”Œë«í¼": ["ì›¹í˜ì´ì§€(ë”ì¿ )"],
                    "ê²Œì‹œë¬¼ URL": [url],
                    "ê²Œì‹œë¬¼ ì œëª©": [cleaned_title],
                    "ê²Œì‹œë¬¼ ë‚´ìš©": [post_content],
                    "ê²Œì‹œë¬¼ ë“±ë¡ì¼ì": [date],
                    "ê³„ì •ëª…": [writer],
                    "ìˆ˜ì§‘ì‹œê°„": [now_time],
                })
                
                file_name = os.path.join(save_path, f'ë”ì¿ _{search}.csv')
                save_to_csv(df, file_name)
                logging.info(f'ì €ì¥ ì™„ë£Œ: {file_name}')

    except Exception as e:
        logging.error(f"ìƒì„¸ í˜ì´ì§€ ì˜¤ë¥˜: {e}")
        print(f"ìƒì„¸ í˜ì´ì§€ ì˜¤ë¥˜: {e}")


def dq_main_crw(searchs, start_date, end_date, stop_event, max_pages=1400):
    target_date = start_date.strftime("%y%m%d")
    
    current_dir = os.path.dirname(__file__)
    project_root = os.path.abspath(os.path.join(current_dir, '..'))
    
    log_dir = os.path.join(project_root, 'log')
    os.makedirs(log_dir, exist_ok=True)

    logging.basicConfig(
        filename=os.path.join(log_dir, f'ë”ì¿ _log_{target_date}.txt'),
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        encoding='utf-8',
        force=True
    )

    logging.info(f"========================================================")
    logging.info(f"             ë”ì¿  í¬ë¡¤ë§ ì‹œì‘ (Date: {target_date})")
    logging.info(f"========================================================")
    
    wd = setup_driver()
    wd_detail = setup_driver()

    page_num = 1
    visited_urls = set()

    while page_num <= max_pages:
        if stop_event.is_set():
            print("ğŸ›‘ í¬ë¡¤ë§ ì¤‘ë‹¨ë¨")
            break
        url_list_page = f'https://theqoo.net/square/category/512000849?page={page_num}'
        logging.info(f"[{page_num}í˜ì´ì§€] ì ‘ì†: {url_list_page}")
        print(f"[{page_num}í˜ì´ì§€] ì ‘ê·¼ ì¤‘...")

        try:
            wd.get(url_list_page)
            time.sleep(random.uniform(2, 3))

            WebDriverWait(wd, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, 'tbody.hide_notice tr'))
            )

            soup = BeautifulSoup(wd.page_source, 'html.parser')
            post_list = soup.select('tbody.hide_notice tr:not(.notice)')

            all_old = True
            stop_flag = False

            for post in post_list:
                if stop_event.is_set():
                    break
                try:
                    date_str = post.select_one('.time').get_text(strip=True)
                    post_date = parse_theqoo_date(date_str)
                    if not post_date:
                        continue

                    if post_date >= start_date:
                        all_old = False
                    if post_date > end_date:
                        continue
                    if post_date < start_date:
                        stop_flag = True
                        continue

                    title_tag = post.select_one('td.title > a:not(.replyNum)')
                    if not title_tag:
                        continue

                    post_url = 'https://theqoo.net' + title_tag.get('href')
                    if post_url in visited_urls:
                        continue
                    visited_urls.add(post_url)

                    # [ìˆ˜ì •] target_date ì „ë‹¬
                    dq_crw(wd_detail, post_url, searchs, target_date)

                except Exception as e:
                    logging.error(f"ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue

            if all_old:
                page_num += 1
                continue

            if stop_flag:
                break

            page_num += 1

        except WebDriverException as e:
            logging.error(f"{page_num}í˜ì´ì§€ WebDriver ì˜ˆì™¸ ë°œìƒ: {e}")
            print(f"âŒ WebDriver ì˜ˆì™¸ ë°œìƒ! ë“œë¼ì´ë²„ ì¬ì‹œì‘ ì¤‘...")
            wd.quit()
            wd = setup_driver()
            page_num += 1
            continue

    wd.quit()
    wd_detail.quit()

    if not stop_event.is_set():
        result_dir = os.path.join(project_root, 'ê²°ê³¼', 'ë”ì¿ ')
        os.makedirs(result_dir, exist_ok=True)

        all_data = pd.concat([
            result_csv_data(search, platform='ë”ì¿ ', subdir=f'24.ë”ì¿ /{target_date}', base_path='data/raw')
            for search in searchs
        ])
        all_data.to_csv(os.path.join(result_dir, f'ë”ì¿ _raw data_{target_date}.csv'), encoding='utf-8', index=False)