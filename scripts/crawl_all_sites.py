import sys
import os
import argparse
import pandas as pd
from datetime import datetime
from threading import Event

# [ì¤‘ìš”] Airflow ë° ë¡œì»¬ í™˜ê²½ ëª¨ë‘ì—ì„œ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# ê° í¬ë¡¤ëŸ¬ ëª¨ë“ˆ ì„í¬íŠ¸
# (ë‹¤ë¥¸ í¬ë¡¤ëŸ¬ë“¤ë„ pp_crawlerì²˜ëŸ¼ ìˆ˜ì •í•´ì£¼ì…”ì•¼ ë‚ ì§œê°€ ì •í™•íˆ ë§ìŠµë‹ˆë‹¤)
from crawlers.pp_crawler import pp_main_crw
from crawlers.clien_crawler import clien_main_crw
from crawlers.inven_crawler import inven_main_crw
from crawlers.todayhumor_crawler import todayhumor_main_crw
from crawlers.paan_crawler import paan_main_crw
from crawlers.instiz_crawler import instiz_main_crw
from crawlers.bobaedream_crawler import bobaedream_main_crw
from crawlers.rw_crawler import rw_main_crw
from crawlers.arca_crawler import arca_main_crw
from crawlers.ilbe_crawler import ilbe_main_crw
from crawlers.humoruniv_crawler import humoruniv_main_crw
from crawlers.cook82_crawler import cook82_main_crw
from crawlers.orbi_crawler import orbi_main_crw
from crawlers.dogdrip_crawler import dogdrip_main_crw
from crawlers.dp_crawler import dp_main_crw
from crawlers.scline_crawler import scline_main_crw
from crawlers.dongsaroma_crawler import dongsaroma_main_crw
from crawlers.fomos_crawler import fomos_main_crw
from crawlers.jjang0u_crawler import jjang0u_main_crw
from crawlers.blind_crawler import blind_main_crw
from crawlers.mlb_crawler import mlb_main_crw
from crawlers.dc_crawler import dc_main_crw
from crawlers.fm_crawler import fm_main_crw
from crawlers.dq_crawler import dq_main_crw

# ì „ì—­ ì¤‘ë‹¨ í”Œë˜ê·¸
stop_event = Event()

# ì‚¬ì´íŠ¸ë³„ í•¨ìˆ˜ ë§¤í•‘
crawlers = {
    "ë½ë¿Œ": pp_main_crw,
    "í´ë¦¬ì•™": clien_main_crw,
    "ì¸ë²¤": inven_main_crw,
    "ë£¨ë¦¬ì›¹": rw_main_crw,
    "ì˜¤ëŠ˜ì˜ìœ ë¨¸": todayhumor_main_crw,
    "ë„¤ì´íŠ¸íŒ": paan_main_crw,
    "ì¸ìŠ¤í‹°ì¦ˆ": instiz_main_crw,
    "ë³´ë°°ë“œë¦¼": bobaedream_main_crw,
    "ì•„ì¹´ë¼ì´ë¸Œ": arca_main_crw,
    "ì¼ê°„ë² ìŠ¤íŠ¸": ilbe_main_crw,
    "ì›ƒê¸´ëŒ€í•™": humoruniv_main_crw,
    "82ì¿¡": cook82_main_crw,
    "ì˜¤ë¥´ë¹„": orbi_main_crw,
    "ê°œë“œë¦½": dogdrip_main_crw,
    "DVDí”„ë¼ì„": dp_main_crw,
    "ì‚¬ì»¤ë¼ì¸": scline_main_crw,
    "ë™ì‚¬ë¡œë§ˆë‹·ì»´": dongsaroma_main_crw,
    "í¬ëª¨ìŠ¤": fomos_main_crw,
    "ì§±ê³µìœ ë‹·ì»´": jjang0u_main_crw,
    "ë¸”ë¼ì¸ë“œ": blind_main_crw,
    "ì— ì—˜ë¹„íŒŒí¬": mlb_main_crw,
    "ë””ì‹œì¸ì‚¬ì´ë“œ": dc_main_crw,
    "ì—í¨ì½”ë¦¬ì•„": fm_main_crw,
    "ë”ì¿ ": dq_main_crw
}

def main(site, start_date, end_date, search_excel):
    print(f"ğŸ”§ [ì„¤ì • í™•ì¸] ì‹œì‘ì¼: {start_date}, ì¢…ë£Œì¼: {end_date}")
    print(f"ğŸ“‚ [ì—‘ì…€ ê²½ë¡œ] {search_excel}")

    # 1. ì—‘ì…€ ë¡œë“œ
    try:
        pd_search = pd.read_excel(search_excel, sheet_name='ê²€ìƒ‰ì–´ ëª©ë¡')
        searchs = pd_search['ê²€ìƒ‰ì–´ëª…']
    except Exception as e:
        print(f"âŒ ì—‘ì…€ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return

    # 2. ë‚ ì§œ ë¬¸ìì—´ -> datetime.date ê°ì²´ë¡œ ë³€í™˜
    # (ì´ ê°ì²´ê°€ pp_crawler.pyë¡œ ë„˜ì–´ê°€ì„œ .strftime('%y%m%d')ë¡œ ë³€í™˜ë©ë‹ˆë‹¤)
    try:
        start_date_obj = datetime.strptime(start_date, "%Y-%m-%d").date()
        end_date_obj = datetime.strptime(end_date, "%Y-%m-%d").date()
    except ValueError:
        print("âŒ ë‚ ì§œ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. YYYY-MM-DD í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        return

    # 3. í¬ë¡¤ë§ ëŒ€ìƒ ì‚¬ì´íŠ¸ ì„ ì •
    sites_to_crawl = []
    if site == "all":
        sites_to_crawl = list(crawlers.keys())
        print(f"ğŸ“¢ [ì „ì²´ ëª¨ë“œ] ì´ {len(sites_to_crawl)}ê°œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    elif site in crawlers:
        sites_to_crawl = [site]
    else:
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸ì…ë‹ˆë‹¤: {site}")
        print(f"   (ì‚¬ìš© ê°€ëŠ¥: 'all' ë˜ëŠ” {', '.join(crawlers.keys())})")
        return

    # 4. í¬ë¡¤ë§ ì‹¤í–‰ ë£¨í”„
    for site_name in sites_to_crawl:
        crawler_func = crawlers[site_name]
        print(f"\nğŸš€ [{site_name}] í¬ë¡¤ë§ ì‹œì‘... (Target: {start_date_obj})")
        
        try:
            # [í•µì‹¬] ì—¬ê¸°ì„œ start_date_obj(ì–´ì œ ë‚ ì§œ ê°ì²´)ë¥¼ ë„˜ê²¨ì¤ë‹ˆë‹¤.
            # pp_crawler.py ë‚´ë¶€ì—ì„œ ì´ ê°ì²´ë¥¼ ë°›ì•„ íŒŒì¼ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤.
            crawler_func(searchs, start_date_obj, end_date_obj, stop_event)
            print(f"âœ… [{site_name}] ìˆ˜ì§‘ ì™„ë£Œ")
            
        except Exception as e:
            # í•˜ë‚˜ê°€ ì‹¤íŒ¨í•´ë„ ë‚˜ë¨¸ì§€ëŠ” ê³„ì† ì§„í–‰ (Airflow ë¡œê·¸ì—ì„œ í™•ì¸ ê°€ëŠ¥)
            print(f"âŒ [{site_name}] í¬ë¡¤ë§ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            # í•„ìš” ì‹œ ì—¬ê¸°ì„œ raiseë¥¼ í•˜ì—¬ Airflow Taskë¥¼ ì‹¤íŒ¨ ì²˜ë¦¬í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
            # raise e 

    print("\nğŸ‰ ëª¨ë“  ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‘ì—… ì¢…ë£Œ")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ì‚¬ì´íŠ¸ë³„ ì»¤ë®¤ë‹ˆí‹° í¬ë¡¤ëŸ¬ ì‹¤í–‰")
    parser.add_argument("--site", required=True, help="ì‚¬ì´íŠ¸ ì´ë¦„ (ì˜ˆ: ë½ë¿Œ, all)")
    parser.add_argument("--start_date", required=True, help="ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)")
    parser.add_argument("--end_date", required=True, help="ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)")
    parser.add_argument("--search_excel", required=True, help="ê²€ìƒ‰ì–´ ì—‘ì…€ íŒŒì¼ ê²½ë¡œ")

    args = parser.parse_args()
    main(args.site, args.start_date, args.end_date, args.search_excel)