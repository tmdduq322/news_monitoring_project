import sys
import os
import argparse
import pandas as pd
from datetime import datetime
import multiprocessing
import time
import glob

# [ì¤‘ìš”] Airflow ë° ë¡œì»¬ í™˜ê²½ ëª¨ë‘ì—ì„œ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# ê° í¬ë¡¤ëŸ¬ ëª¨ë“ˆ ì„í¬íŠ¸
from crawlers.pp_crawler import pp_main_crw
from crawlers.clien_crawler import clien_main_crw
from crawlers.inven_crawler import inven_main_crw
from crawlers.todayhumor_crawler import todayhumor_main_crw
from crawlers.paan_crawler import paan_main_crw
from crawlers.instiz_crawler import instiz_main_crw
from crawlers.bobaedream_crawler import bobaedream_main_crw
from crawlers.rw_crawler import rw_main_crw
from crawlers.arca_crawler import arca_main_crw
from crawlers.ilbe_crawler import ilbe_main_crw
from crawlers.humoruniv_crawler import humoruniv_main_crw
from crawlers.cook82_crawler import cook82_main_crw
from crawlers.orbi_crawler import orbi_main_crw
from crawlers.dogdrip_crawler import dogdrip_main_crw
from crawlers.dp_crawler import dp_main_crw
from crawlers.dongsaroma_crawler import dongsaroma_main_crw
from crawlers.scline_crawler import scline_main_crw
from crawlers.fomos_crawler import fomos_main_crw
from crawlers.jjang0u_crawler import jjang0u_main_crw
from crawlers.blind_crawler import blind_main_crw
from crawlers.mlb_crawler import mlb_main_crw
from crawlers.dc_crawler import dc_main_crw
from crawlers.fm_crawler import fm_main_crw
from crawlers.dq_crawler import dq_main_crw

# í¬ë¡¤ëŸ¬ ë§¤í•‘
crawlers = {
    "ë½ë¿Œ": pp_main_crw,
    "í´ë¦¬ì•™": clien_main_crw,
    "ì¸ë²¤": inven_main_crw,
    "ì˜¤ëŠ˜ì˜ìœ ë¨¸": todayhumor_main_crw,
    "ë„¤ì´íŠ¸íŒ": paan_main_crw,
    "ì¸ìŠ¤í‹°ì¦ˆ": instiz_main_crw,
    "ë³´ë°°ë“œë¦¼": bobaedream_main_crw,
    "ë£¨ë¦¬ì›¹": rw_main_crw,
    "ì•„ì¹´ë¼ì´ë¸Œ": arca_main_crw,
    "ì¼ê°„ë² ìŠ¤íŠ¸": ilbe_main_crw,
    "ì›ƒê¸´ëŒ€í•™": humoruniv_main_crw,
    "82ì¿¡": cook82_main_crw,
    "ì˜¤ë¥´ë¹„": orbi_main_crw,
    "ê°œë“œë¦½": dogdrip_main_crw,
    "DVDí”„ë¼ì„": dp_main_crw,
    "ë™ì‚¬ë¡œë§ˆë‹·ì»´": dongsaroma_main_crw,
    "ì‚¬ì»¤ë¼ì¸": scline_main_crw,
    "í¬ëª¨ìŠ¤": fomos_main_crw,
    "ì§±ê³µìœ ë‹·ì»´": jjang0u_main_crw,
    "ë¸”ë¼ì¸ë“œ": blind_main_crw,
    "ì— ì—˜ë¹„íŒŒí¬": mlb_main_crw,
    "ë””ì‹œì¸ì‚¬ì´ë“œ": dc_main_crw,
    "ì—í¨ì½”ë¦¬ì•„": fm_main_crw,
    "ë”ì¿ ": dq_main_crw
}

# í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ë˜í¼ í•¨ìˆ˜
def run_crawler_process(crawler_func, searchs, start_date, end_date, stop_event):
    try:
        crawler_func(searchs, start_date, end_date, stop_event)
    except Exception as e:
        print(f"Error inside process: {e}")

# [í•µì‹¬] í•´ë‹¹ ì‚¬ì´íŠ¸ì˜ ë°ì´í„° ì €ì¥ í´ë” ì°¾ê¸°
def find_data_folder(site_name, target_date_str):
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ data/raw ê²½ë¡œ
    base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'data', 'raw'))
    
    # 1.ë½ë¿Œ, 5.ì˜¤ëŠ˜ì˜ìœ ë¨¸ ë“± í´ë”ëª…ì´ ìˆ«ìì™€ ì„ì—¬ìˆìœ¼ë¯€ë¡œ ê²€ìƒ‰
    if not os.path.exists(base_dir):
        return None
        
    for folder in os.listdir(base_dir):
        if site_name in folder: 
            full_path = os.path.join(base_dir, folder, target_date_str)
            return full_path
    return None

# [í•µì‹¬] í´ë” ë‚´ì—ì„œ ê°€ì¥ ìµœì‹  íŒŒì¼ì˜ ìˆ˜ì • ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
def get_latest_file_mtime(folder_path):
    if not folder_path or not os.path.exists(folder_path):
        return 0
    
    list_of_files = glob.glob(os.path.join(folder_path, '*.csv'))
    if not list_of_files:
        return 0
    
    latest_file = max(list_of_files, key=os.path.getmtime)
    return os.path.getmtime(latest_file)

if __name__ == "__main__":
    multiprocessing.freeze_support()

    parser = argparse.ArgumentParser()
    parser.add_argument("--site", type=str, default="all", help="í¬ë¡¤ë§í•  ì‚¬ì´íŠ¸ ì´ë¦„ (ì½¤ë§ˆë¡œ êµ¬ë¶„ ê°€ëŠ¥, ì˜ˆ: 'ë½ë¿Œ,í´ë¦¬ì•™')")
    parser.add_argument("--start_date", type=str, required=True, help="ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)")
    parser.add_argument("--end_date", type=str, required=True, help="ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)")
    parser.add_argument("--search_excel", type=str, required=True, help="ê²€ìƒ‰ì–´ ì—‘ì…€ íŒŒì¼ ê²½ë¡œ")
    
    args = parser.parse_args()

    # 1. ë‚ ì§œ ë³€í™˜
    try:
        start_date_obj = datetime.strptime(args.start_date, "%Y-%m-%d").date()
        end_date_obj = datetime.strptime(args.end_date, "%Y-%m-%d").date()
        target_date_str = start_date_obj.strftime("%y%m%d") # í´ë”ëª…ìš© (251222)
    except ValueError:
        print("âŒ ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        sys.exit(1)

    # 2. ê²€ìƒ‰ì–´ ë¡œë“œ
    if not os.path.exists(args.search_excel):
        print(f"âŒ ê²€ìƒ‰ì–´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.search_excel}")
        sys.exit(1)
    
    df = pd.read_excel(args.search_excel, sheet_name='ê²€ìƒ‰ì–´ ëª©ë¡')
    searchs = df['ê²€ìƒ‰ì–´ëª…'].tolist()

    # 3. ì‚¬ì´íŠ¸ ì„ íƒ (ë‹¤ì¤‘ ì‚¬ì´íŠ¸ ì§€ì› ìˆ˜ì •)
    input_site = args.site
    sites_to_crawl = []

    if input_site == "all":
        sites_to_crawl = list(crawlers.keys())
    else:
        # ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ (ê³µë°± ì œê±° í¬í•¨)
        potential_sites = [s.strip() for s in input_site.split(',')]
        for s in potential_sites:
            if s in crawlers:
                sites_to_crawl.append(s)
            else:
                print(f"âš ï¸ ê²½ê³ : '{s}'ëŠ”(ì€) ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸ì´ê±°ë‚˜ ì˜¤íƒ€ì…ë‹ˆë‹¤.")

    if not sites_to_crawl:
        print("âŒ ì‹¤í–‰í•  ìœ íš¨í•œ ì‚¬ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    print(f"ğŸ“‹ í¬ë¡¤ë§ ëŒ€ìƒ ì‚¬ì´íŠ¸ ({len(sites_to_crawl)}ê°œ): {sites_to_crawl}")

    # [ì„¤ì •] ë¬´ì‘ë‹µ ëŒ€ê¸° ì‹œê°„ (5ë¶„) - í•„ìš”ì‹œ ì¡°ì •
    IDLE_TIMEOUT = 4 * 60 
    # [ì„¤ì •] ì „ì²´ ìµœëŒ€ ì œí•œ ì‹œê°„ (6ì‹œê°„)
    MAX_TOTAL_TIMEOUT = 6 * 60 * 60

    stop_event = multiprocessing.Event()

    for site_name in sites_to_crawl:
        crawler_func = crawlers[site_name]
        print(f"\nğŸš€ [{site_name}] í¬ë¡¤ë§ ì‹œì‘... ({IDLE_TIMEOUT}ì´ˆ ë¬´ì‘ë‹µ ì‹œ ì¢…ë£Œ)")
        
        p = multiprocessing.Process(
            target=run_crawler_process, 
            args=(crawler_func, searchs, start_date_obj, end_date_obj, stop_event)
        )
        
        p.start()
        
        process_start_time = time.time()
        last_activity_time = time.time()
        
        # ê°ì‹œí•  í´ë” ê²½ë¡œ ì°¾ê¸° (ì´ˆê¸°ì—” ì—†ì„ ìˆ˜ ìˆìŒ)
        target_folder = None
        
        while p.is_alive():
            current_time = time.time()
            
            # 1. ì „ì²´ ì‹œê°„ ì´ˆê³¼ ì²´í¬ (ì•ˆì „ì¥ì¹˜)
            if current_time - process_start_time > MAX_TOTAL_TIMEOUT:
                print(f"ğŸ›‘ [{site_name}] ì „ì²´ ì œí•œ ì‹œê°„({MAX_TOTAL_TIMEOUT}ì´ˆ) ì´ˆê³¼! ê°•ì œ ì¢…ë£Œ.")
                p.terminate()
                break

            # 2. í´ë” ì°¾ê¸° (ì•„ì§ ì•ˆ ë§Œë“¤ì–´ì¡Œì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë°˜ë³µ ì‹œë„)
            if target_folder is None:
                target_folder = find_data_folder(site_name, target_date_str)
            
            # 3. íŒŒì¼ ë³€ê²½ ì‹œê°„ í™•ì¸ (Idle Check)
            latest_file_time = get_latest_file_mtime(target_folder)
            
            # ë§Œì•½ íŒŒì¼ì´ ìˆ˜ì •ë˜ì—ˆê±°ë‚˜ ìƒˆë¡œ ìƒê²¼ìœ¼ë©´ -> í™œë™ ì¤‘! ì‹œê°„ ê°±ì‹ 
            if latest_file_time > last_activity_time:
                last_activity_time = latest_file_time
                # print(f"   [{site_name}] ìƒˆ ë°ì´í„° ê°ì§€ë¨! íƒ€ì´ë¨¸ ë¦¬ì…‹.") 

            # 4. ë¬´ì‘ë‹µ ì‹œê°„ ì²´í¬
            idle_duration = current_time - last_activity_time
            if idle_duration > IDLE_TIMEOUT:
                print(f"â° [{site_name}] {IDLE_TIMEOUT/60:.1f}ë¶„ ë™ì•ˆ ìƒˆ ë°ì´í„° ì—†ìŒ! (ì •ì²´ë¨) -> ë‹¤ìŒ ì‚¬ì´íŠ¸ë¡œ ì´ë™.")
                p.terminate()
                p.join()
                break
            
            # 5ì´ˆë§ˆë‹¤ ê²€ì‚¬
            time.sleep(5)

        p.join() # ì¢€ë¹„ í”„ë¡œì„¸ìŠ¤ ë°©ì§€

        if p.exitcode == 0:
            print(f"âœ… [{site_name}] ì‘ì—… ì™„ë£Œ")
        else:
            print(f"âš ï¸ [{site_name}] ì‘ì—… ì¢…ë£Œë¨ (Exit Code: {p.exitcode})")

    print("\nğŸ‰ ì§€ì •ëœ ëª¨ë“  ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‘ì—… ì¢…ë£Œ")