import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
import pandas as pd
from datetime import datetime
from threading import Event
import argparse
from crawlers.pp_crawler import pp_main_crw
from crawlers.clien_crawler import clien_main_crw
from crawlers.inven_crawler import inven_main_crw
from crawlers.todayhumor_crawler import todayhumor_main_crw
from crawlers.paan_crawler import paan_main_crw
from crawlers.instiz_crawler import instiz_main_crw
from crawlers.bobaedream_crawler import bobaedream_main_crw
from crawlers.rw_crawler import rw_main_crw
from crawlers.arca_crawler import arca_main_crw
from crawlers.ilbe_crawler import ilbe_main_crw
from crawlers.humoruniv_crawler import humoruniv_main_crw
from crawlers.cook82_crawler import cook82_main_crw
from crawlers.orbi_crawler import orbi_main_crw
from crawlers.dogdrip_crawler import dogdrip_main_crw
from crawlers.dp_crawler import dp_main_crw
from crawlers.scline_crawler import scline_main_crw
from crawlers.dongsaroma_crawler import dongsaroma_main_crw
from crawlers.fomos_crawler import fomos_main_crw
from crawlers.jjang0u_crawler import jjang0u_main_crw
from crawlers.blind_crawler import blind_main_crw
from crawlers.mlb_crawler import mlb_main_crw
from crawlers.dc_crawler import dc_main_crw
from crawlers.fm_crawler import fm_main_crw
from crawlers.dq_crawler import dq_main_crw

# ì „ì—­ ì¤‘ë‹¨ í”Œë˜ê·¸
stop_event = Event()

# ì‚¬ì´íŠ¸ë³„ í•¨ìˆ˜ ë§¤í•‘
crawlers = {
    "ë½ë¿Œ": pp_main_crw,
    "í´ë¦¬ì•™": clien_main_crw,
    "ì¸ë²¤": inven_main_crw,
    "ë£¨ë¦¬ì›¹": rw_main_crw,
    "ì˜¤ëŠ˜ì˜ìœ ë¨¸": todayhumor_main_crw,
    "ë„¤ì´íŠ¸íŒ": paan_main_crw,
    "ì¸ìŠ¤í‹°ì¦ˆ": instiz_main_crw,
    "ë³´ë°°ë“œë¦¼": bobaedream_main_crw,
    "ì•„ì¹´ë¼ì´ë¸Œ": arca_main_crw,
    "ì¼ê°„ë² ìŠ¤íŠ¸": ilbe_main_crw,
    "ì›ƒê¸´ëŒ€í•™": humoruniv_main_crw,
    "82ì¿¡": cook82_main_crw,
    "ì˜¤ë¥´ë¹„": orbi_main_crw,
    "ê°œë“œë¦½": dogdrip_main_crw,
    "DVDí”„ë¼ì„": dp_main_crw,
    "ì‚¬ì»¤ë¼ì¸": scline_main_crw,
    "ë™ì‚¬ë¡œë§ˆë‹·ì»´": dongsaroma_main_crw,
    "í¬ëª¨ìŠ¤": fomos_main_crw,
    "ì§±ê³µìœ ë‹·ì»´": jjang0u_main_crw,
    "ë¸”ë¼ì¸ë“œ": blind_main_crw,
    "ì— ì—˜ë¹„íŒŒí¬": mlb_main_crw,
    "ë””ì‹œì¸ì‚¬ì´ë“œ": dc_main_crw,
    "ì—í¨ì½”ë¦¬ì•„": fm_main_crw,
    "ë”ì¿ ": dq_main_crw
}

def main(site, start_date, end_date, search_excel):
    pd_search = pd.read_excel(search_excel, sheet_name='ê²€ìƒ‰ì–´ ëª©ë¡')
    searchs = pd_search['ê²€ìƒ‰ì–´ëª…']
    start_date = datetime.strptime(start_date, "%Y-%m-%d").date()
    end_date = datetime.strptime(end_date, "%Y-%m-%d").date()

    if site not in crawlers:
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸ì…ë‹ˆë‹¤: {site}")
        return

    print(f"ğŸš€ [{site}] í¬ë¡¤ë§ ì‹œì‘...")
    crawlers[site](searchs, start_date, end_date, stop_event)
    print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ì‚¬ì´íŠ¸ë³„ ì»¤ë®¤ë‹ˆí‹° í¬ë¡¤ëŸ¬ ì‹¤í–‰")
    parser.add_argument("--site", required=True, help="ì‚¬ì´íŠ¸ ì´ë¦„ (ì˜ˆ: ë£¨ë¦¬ì›¹, ë³´ë°°ë“œë¦¼ ë“±)")
    parser.add_argument("--start_date", required=True, help="ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)")
    parser.add_argument("--end_date", required=True, help="ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)")
    parser.add_argument("--search_excel", required=True, help="ê²€ìƒ‰ì–´ ì—‘ì…€ íŒŒì¼ ê²½ë¡œ")

    args = parser.parse_args()
    main(args.site, args.start_date, args.end_date, args.search_excel)
