import os
import argparse
import pandas as pd
from sqlalchemy import create_engine, text
from sqlalchemy.exc import IntegrityError
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
SCRIPT_PATH = os.path.abspath(__file__)
PROJECT_ROOT_DIR = os.path.dirname(os.path.dirname(SCRIPT_PATH))

# .env ë¡œë“œ (AWS RDS ì ‘ì† ì •ë³´)
from dotenv import load_dotenv
load_dotenv(os.path.join(PROJECT_ROOT_DIR, '.env'))

def save_to_mysql(input_file, table_name):
    # 1. DB ì—°ê²° ë¬¸ìì—´ ìƒì„±
    db_user = os.getenv("DB_USER", "admin")
    db_password = os.getenv("DB_PASSWORD")
    db_host = os.getenv("DB_HOST")
    db_name = os.getenv("DB_NAME", "airflow_db")
    
    if not db_password or not db_host:
        print("âŒ .env íŒŒì¼ì— DB ì ‘ì† ì •ë³´(DB_HOST, DB_PASSWORD)ê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    # SQLAlchemy ì—”ì§„ ìƒì„±
    db_url = f"mysql+mysqldb://{db_user}:{db_password}@{db_host}:3306/{db_name}?charset=utf8mb4"
    engine = create_engine(db_url)

    # 2. ë°ì´í„° ë¡œë“œ
    print(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘: {input_file}")
    try:
        # S3 ê²½ë¡œì¸ ê²½ìš° storage_options í•„ìš” (s3fs ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”)
        if input_file.startswith("s3://"):
            storage_options = {
                "key": os.getenv("AWS_ACCESS_KEY_ID"),
                "secret": os.getenv("AWS_SECRET_ACCESS_KEY")
            }
            df = pd.read_csv(input_file, storage_options=storage_options)
        else:
            df = pd.read_csv(input_file)
            
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        sys.exit(1)

    if df.empty:
        print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 3. í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ ë° ì •ì œ
    # DB ìŠ¤í‚¤ë§ˆì™€ DataFrame ì»¬ëŸ¼ëª…ì„ ë§ì¶°ì£¼ëŠ” ì‘ì—…ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # ì˜ˆ: 'ê²Œì‹œë¬¼ ì œëª©' -> 'title', 'ê²Œì‹œë¬¼ URL' -> 'url' ë“±
    # ì—¬ê¸°ì„œëŠ” CSV ì»¬ëŸ¼ëª…ì„ ê·¸ëŒ€ë¡œ ì“´ë‹¤ê³  ê°€ì •í•˜ê±°ë‚˜, ë§¤í•‘í•©ë‹ˆë‹¤.
    column_mapping = {
        "ê²Œì‹œë¬¼ ì œëª©": "title",
        "ê²Œì‹œë¬¼ ë‚´ìš©": "content",
        "ê²Œì‹œë¬¼ URL": "url",
        "ê²Œì‹œë¬¼ ë“±ë¡ì¼ì": "published_at",
        "ìˆ˜ì§‘ì‹œê°„": "crawled_at",
        "í”Œë«í¼": "platform",
        "ê³„ì •ëª…": "writer",
        "ì›ë³¸ê¸°ì‚¬": "original_article_url",
        "ë³µì‚¬ìœ¨": "copy_rate"
    }
    
    # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ë³€ê²½
    df = df.rename(columns=column_mapping)
    
    # DBì— ì—†ëŠ” ì»¬ëŸ¼ì´ dfì— ìˆìœ¼ë©´ ì—ëŸ¬ë‚˜ë¯€ë¡œ, í•„ìš”í•œ ì»¬ëŸ¼ë§Œ í•„í„°ë§í•˜ëŠ” ë¡œì§ ì¶”ì²œ
    # (ì—¬ê¸°ì„œëŠ” ìƒëµí•˜ê³  ì§„í–‰)

    print(f"ğŸ’¾ DB ì €ì¥ ì‹œì‘ ({len(df)}ê±´) -> í…Œì´ë¸”: {table_name}")

    # 4. ë°ì´í„° ì €ì¥ (INSERT IGNORE ë°©ì‹ êµ¬í˜„)
    # Pandasì˜ to_sqlì€ ê¸°ë³¸ì ìœ¼ë¡œ ì¤‘ë³µ ì²˜ë¦¬ë¥¼ ëª»í•˜ë¯€ë¡œ, temp í…Œì´ë¸”ì„ í™œìš©í•˜ê±°ë‚˜
    # í•œ ì¤„ì”© ë„£ìœ¼ë©´ì„œ ì˜ˆì™¸ì²˜ë¦¬ë¥¼ í•´ì•¼ í•©ë‹ˆë‹¤. ëŒ€ëŸ‰ ë°ì´í„°ì—ëŠ” temp í…Œì´ë¸” ë°©ì‹ì´ ë¹ ë¦…ë‹ˆë‹¤.
    
    try:
        with engine.connect() as conn:
            # (1) URL ì»¬ëŸ¼ì— ìœ ë‹ˆí¬ ì¸ë±ìŠ¤ê°€ ì—†ë‹¤ë©´ ìƒì„± (ìµœì´ˆ 1íšŒë§Œ ì‹¤í–‰ë¨)
            # í¬íŠ¸í´ë¦¬ì˜¤ìš©ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì½”ë“œ ë‚´ì—ì„œ ì²˜ë¦¬
            try:
                conn.execute(text(f"ALTER TABLE {table_name} ADD UNIQUE INDEX idx_url (url(255));"))
                print("âœ… URL ì»¬ëŸ¼ì— ìœ ë‹ˆí¬ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
            except Exception:
                pass # ì´ë¯¸ ìˆìœ¼ë©´ íŒ¨ìŠ¤

            # (2) Pandas to_sqlë¡œ 'append' (ì¤‘ë³µë‚˜ë©´ ì—ëŸ¬ ë°œìƒí•¨)
            # ë”°ë¼ì„œ 'chunksize'ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‚˜ëˆ„ì–´ ë„£ê±°ë‚˜, 
            # ê°€ì¥ ê¹”ë”í•œ ë°©ë²•: 'INSERT IGNORE' ì¿¼ë¦¬ë¥¼ ì§ì ‘ ìƒì„±í•´ì„œ ì‹¤í–‰
            
            # DataFrameì„ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            data_to_insert = df.to_dict(orient='records')
            
            success_count = 0
            
            # ì¿¼ë¦¬ë¬¸ ìƒì„± (MySQL INSERT IGNORE)
            # ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
            if not data_to_insert:
                return
            
            columns = data_to_insert[0].keys()
            cols_str = ", ".join([f"`{c}`" for c in columns])
            vals_str = ", ".join([f":{c}" for c in columns])
            
            sql = text(f"INSERT IGNORE INTO {table_name} ({cols_str}) VALUES ({vals_str})")
            
            # ì‹¤í–‰
            result = conn.execute(sql, data_to_insert)
            conn.commit()
            
            print(f"âœ… DB ì €ì¥ ì™„ë£Œ. (ì˜í–¥ë°›ì€ í–‰: {result.rowcount}ê°œ / ì „ì²´: {len(df)}ê°œ)")
            print("   (ì¤‘ë³µëœ URLì€ ìë™ìœ¼ë¡œ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤)")

    except Exception as e:
        print(f"âŒ DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # í…Œì´ë¸”ì´ ì•„ì˜ˆ ì—†ì–´ì„œ ì—ëŸ¬ë‚œ ê²½ìš°ë¼ë©´, to_sqlë¡œ ìµœì´ˆ ìƒì„± ì‹œë„
        if "Table" in str(e) and "doesn't exist" in str(e):
            print("âš ï¸ í…Œì´ë¸”ì´ ì—†ì–´ì„œ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            df.to_sql(table_name, engine, if_exists='replace', index=False)
            # ìƒì„± í›„ ìœ ë‹ˆí¬ ì¸ë±ìŠ¤ ì¶”ê°€
            with engine.connect() as conn:
                conn.execute(text(f"ALTER TABLE {table_name} ADD UNIQUE INDEX idx_url (url(255));"))
            print("âœ… í…Œì´ë¸” ìƒì„± ë° ë°ì´í„° ì €ì¥ ì™„ë£Œ.")
        else:
            sys.exit(1)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_file", required=True, help="ì €ì¥í•  CSV/Excel íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--table_name", default="news_posts", help="ì €ì¥í•  í…Œì´ë¸” ì´ë¦„")
    
    args = parser.parse_args()
    
    # ì—‘ì…€ íŒŒì¼ì¸ ê²½ìš° ë³€í™˜
    if args.input_file.endswith(".xlsx"):
        # ì—‘ì…€ ì½ê¸° ê¸°ëŠ¥ì´ í•„ìš”í•˜ë‹¤ë©´ pandas read_excel ì‚¬ìš©
        # ì—¬ê¸°ì„œëŠ” csvë¡œ ë„˜ì–´ì˜¨ë‹¤ê³  ê°€ì •í•˜ê±°ë‚˜, ì½”ë“œ ìƒë‹¨ì—ì„œ ì²˜ë¦¬
        pass 

    save_to_mysql(args.input_file, args.table_name)