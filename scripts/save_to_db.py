import os
import argparse
import pandas as pd
import glob
import sys
import ssl
from sqlalchemy import create_engine, text

# [í•„ìˆ˜] S3 íŒŒì¼ ê²€ìƒ‰ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import s3fs
except ImportError:
    print("âŒ s3fs ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. 'pip install s3fs'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    sys.exit(1)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
SCRIPT_PATH = os.path.abspath(__file__)
PROJECT_ROOT_DIR = os.path.dirname(os.path.dirname(SCRIPT_PATH))

# .env ë¡œë“œ
from dotenv import load_dotenv
load_dotenv(os.path.join(PROJECT_ROOT_DIR, '.env'))

def save_to_mysql(input_file_pattern, table_name):
    # 1. DB ì—°ê²°
    db_user = os.getenv("DB_USER", "admin")
    db_password = os.getenv("DB_PASSWORD")
    db_host = os.getenv("DB_HOST")
    db_name = os.getenv("DB_NAME", "airflow_db")
    
    if not db_password or not db_host:
        print("âŒ .env íŒŒì¼ì— DB ì ‘ì† ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    # SSL ì„¤ì •
    db_url = f"mysql+mysqldb://{db_user}:{db_password}@{db_host}:3306/{db_name}?charset=utf8mb4"
    connect_args = {
        "ssl": {    
            "check_hostname": False,
            "verify_mode": ssl.CERT_NONE
        }
    }
    engine = create_engine(db_url, connect_args=connect_args)

    # 2. ë°ì´í„° íŒŒì¼ ê²€ìƒ‰
    print(f"ğŸ” íŒŒì¼ ê²€ìƒ‰ ìš”ì²­: {input_file_pattern}")
    
    matched_files = []
    aws_key = os.getenv("AWS_ACCESS_KEY_ID")
    aws_secret = os.getenv("AWS_SECRET_ACCESS_KEY")
    
    # (A) S3 ê²½ë¡œì¸ ê²½ìš°
    if input_file_pattern.startswith("s3://"):
        try:
            fs = s3fs.S3FileSystem(key=aws_key, secret=aws_secret)
            if fs.exists(input_file_pattern):
                matched_files = [input_file_pattern]
            else:
                base, ext = os.path.splitext(input_file_pattern)
                s3_glob_pattern = f"{base}_part_*{ext}"
                if s3_glob_pattern.startswith("s3://"):
                    search_path = s3_glob_pattern[5:]
                else:
                    search_path = s3_glob_pattern
                files = fs.glob(search_path)
                matched_files = [f"s3://{f}" for f in files]
                
                if matched_files:
                    print(f"â„¹ï¸ S3 ë¶„í•  íŒŒì¼ ë°œê²¬ ({len(matched_files)}ê°œ): {matched_files}")
        except Exception as e:
            print(f"âŒ S3 ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            sys.exit(1)

    # (B) ë¡œì»¬ íŒŒì¼ì¸ ê²½ìš°
    else:
        matched_files = glob.glob(input_file_pattern)
        if not matched_files:
            base, ext = os.path.splitext(input_file_pattern)
            part_pattern = f"{base}_part_*{ext}"
            matched_files = glob.glob(part_pattern)

    if not matched_files:
        print(f"âš ï¸ ì €ì¥í•  íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {input_file_pattern}")
        # íŒŒì¼ì´ ì—†ëŠ” ê²ƒì€ ì •ìƒ ìƒí™©ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì—ëŸ¬ ì•„ë‹˜
        return

    # 3. ë°ì´í„° ì½ê¸° ë° ë³‘í•©
    df_list = []
    storage_options = {"key": aws_key, "secret": aws_secret} if aws_key else None

    for file_path in matched_files:
        try:
            if file_path.startswith("s3://"):
                d = pd.read_csv(file_path, storage_options=storage_options)
            else:
                d = pd.read_csv(file_path)
            df_list.append(d)
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({file_path}): {e}")
            sys.exit(1)

    if not df_list:
        return

    df = pd.concat(df_list, ignore_index=True)
    
    # [ì¤‘ìš”] NaN ê°’ì„ NULLë¡œ ë³€í™˜
    df = df.where(pd.notnull(df), None)

    # 4. ì»¬ëŸ¼ ë§¤í•‘
    column_mapping = {
        "ê²Œì‹œë¬¼ ì œëª©": "title", "ê²Œì‹œë¬¼ ë‚´ìš©": "content", "ê²Œì‹œë¬¼ URL": "url",
        "ê²Œì‹œë¬¼ ë“±ë¡ì¼ì": "published_at", "ìˆ˜ì§‘ì‹œê°„": "crawled_at", "í”Œë«í¼": "platform",
        "ê³„ì •ëª…": "writer", "ì›ë³¸ê¸°ì‚¬": "original_article_url", "ë³µì‚¬ìœ¨": "copy_rate",
        "ê²€ìƒ‰ì–´": "keyword" 
    }
    # ì‹¤ì œ ì»¬ëŸ¼ëª…ì— ë§ì¶°ì„œ ë³€ê²½ (ì—‘ì…€/CSV í—¤ë” í™•ì¸ í•„ìš”)
    # ì½”ë“œì—ì„œëŠ” 'ê²€ìƒ‰ì–´'ê°€ ë“¤ì–´ê°€ ìˆëŠ”ì§€ í™•ì¸ í•„ìš”, ì—†ìœ¼ë©´ ì—ëŸ¬ë‚  ìˆ˜ ìˆìŒ
    # dfì— ìˆëŠ” ì»¬ëŸ¼ë§Œ renameí•˜ë„ë¡ ì²˜ë¦¬
    rename_map = {k: v for k, v in column_mapping.items() if k in df.columns}
    df = df.rename(columns=rename_map)
    
    print(f"ğŸ’¾ DB ì €ì¥ ì‹œì‘ (ì´ {len(df)}ê±´) -> í…Œì´ë¸”: {table_name}")

    try:
        # ğŸ‘‡ [ìˆ˜ì • 1] engine.begin() ì‚¬ìš© (ìë™ ì»¤ë°‹ Transaction)
        with engine.begin() as conn:
            # ìœ ë‹ˆí¬ ì¸ë±ìŠ¤ ìƒì„± (ì—†ìœ¼ë©´)
            try:
                conn.execute(text(f"ALTER TABLE {table_name} ADD UNIQUE INDEX idx_url (url(255));"))
            except Exception:
                pass

            data_to_insert = df.to_dict(orient='records')
            if not data_to_insert: return
            
            columns = data_to_insert[0].keys()
            cols_str = ", ".join([f"`{c}`" for c in columns])
            vals_str = ", ".join([f":{c}" for c in columns])
            
            sql = text(f"INSERT IGNORE INTO {table_name} ({cols_str}) VALUES ({vals_str})")
            
            # ì‹¤í–‰ (commitì€ with ë¸”ë¡ ë‚˜ê°ˆ ë•Œ ìë™ ìˆ˜í–‰ë¨)
            result = conn.execute(sql, data_to_insert)
            
            # ğŸ‘‡ [ìˆ˜ì • 2] conn.commit() ì‚­ì œë¨
            print(f"âœ… DB ì €ì¥ ì™„ë£Œ: {result.rowcount}ê±´ ì‚½ì…ë¨.")
            
    except Exception as e:
        print(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
        # í…Œì´ë¸” ì—†ìœ¼ë©´ ìƒì„± ì‹œë„
        if "Table" in str(e) and "doesn't exist" in str(e):
             print("âš ï¸ í…Œì´ë¸” ìƒì„± í›„ ì¬ì‹œë„...")
             df.to_sql(table_name, engine, if_exists='replace', index=False)
             with engine.begin() as conn:
                conn.execute(text(f"ALTER TABLE {table_name} ADD UNIQUE INDEX idx_url (url(255));"))
             print("âœ… í…Œì´ë¸” ìƒì„± ì™„ë£Œ.")
        else:
             # ğŸ‘‡ [ìˆ˜ì • 3] ì§„ì§œ ì—ëŸ¬ë¼ë©´ Airflowê°€ ì•Œ ìˆ˜ ìˆê²Œ ê°•ì œ ì¢…ë£Œ
             sys.exit(1)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_file", required=True)
    parser.add_argument("--table_name", default="news_posts")
    args = parser.parse_args()
    save_to_mysql(args.input_file, args.table_name)