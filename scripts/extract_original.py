import argparse
import os
import sys
import pandas as pd
import numpy as np
from extraction.main_script import find_original_article_multiprocess
from extraction.core_utils import log
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, as_completed

# 1. í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì ˆëŒ€ ê²½ë¡œ ì„¤ì •
SCRIPT_PATH = os.path.abspath(__file__)
PROJECT_ROOT_DIR = os.path.dirname(os.path.dirname(SCRIPT_PATH))

# AWS ì¸ì¦ ì •ë³´
AWS_ACCESS_KEY = os.getenv("AWS_ACCESS_KEY_ID")
AWS_SECRET_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")

def resolve_path(path):
    if path.startswith("s3://"): 
        return path
    if os.path.isabs(path):
        return path
    return os.path.join(PROJECT_ROOT_DIR, path)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ì›ë¬¸ê¸°ì‚¬ ë§¤ì¹­ (ë¶„ì‚° ì²˜ë¦¬ìš©)")
    parser.add_argument("--input_excel", required=True, help="ì „ì²˜ë¦¬ëœ ì…ë ¥ ì—‘ì…€ ê²½ë¡œ")
    parser.add_argument("--output_csv", required=True, help="ê²°ê³¼ ì €ì¥ csv ê²½ë¡œ (ìë™ìœ¼ë¡œ _partX ë¶™ìŒ)")
    # [ì¶”ê°€] ë¶„í•  ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¸ì
    parser.add_argument("--worker_id", type=int, default=0, help="í˜„ì¬ ì›Œì»¤ ë²ˆí˜¸ (0ë¶€í„° ì‹œì‘)")
    parser.add_argument("--total_workers", type=int, default=1, help="ì´ ì›Œì»¤ ìˆ˜")

    args = parser.parse_args()

    input_path = resolve_path(args.input_excel)
    # ì €ì¥ íŒŒì¼ëª… ë¶„ë¦¬ (ì˜ˆ: result.csv -> result_part_0.csv)
    base, ext = os.path.splitext(resolve_path(args.output_csv))
    part_output_path = f"{base}_part_{args.worker_id}{ext}"
    
    # ì„ì‹œ ì €ì¥ íŒŒì¼ë„ ë¶„ë¦¬
    temp_output_path = os.path.join(PROJECT_ROOT_DIR, "data", "extracted", f"temp_progress_part_{args.worker_id}.csv")
    os.makedirs(os.path.dirname(temp_output_path), exist_ok=True)

    if not os.path.exists(input_path):
        log(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")
        sys.exit(1)

    log(f"ğŸ“‚ [Worker {args.worker_id}/{args.total_workers}] íŒŒì¼ ë¡œë“œ: {input_path}")
    try:
        df_all = pd.read_excel(input_path)
    except Exception as e:
        log(f"âŒ ì—‘ì…€ ë¡œë“œ ì‹¤íŒ¨: {e}")
        sys.exit(1)

    # [í•µì‹¬] ë°ì´í„° ë¶„í•  (Partitioning)
    # ì „ì²´ ë°ì´í„°ë¥¼ ì›Œì»¤ ìˆ˜ë§Œí¼ ìª¼ê°œì„œ ë‚´ ëª«ë§Œ ê°€ì ¸ì˜´
    chunks = np.array_split(df_all, args.total_workers)
    if args.worker_id >= len(chunks):
        log("âš ï¸ í• ë‹¹ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        sys.exit(0)
        
    df = chunks[args.worker_id].copy()  # ë‚´ í• ë‹¹ëŸ‰
    
    # ì´ì–´í•˜ê¸° ê¸°ëŠ¥ (ë‚´ íŒŒíŠ¸ì˜ ì„ì‹œ íŒŒì¼ í™•ì¸)
    processed_indices = set()
    if os.path.exists(temp_output_path):
        try:
            df_temp = pd.read_csv(temp_output_path)
            if "ì›ë³¸ê¸°ì‚¬" in df_temp.columns:
                # ì¸ë±ìŠ¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
                # ì£¼ì˜: df_tempëŠ” ì „ì²´ê°€ ì•„ë‹ˆë¼ ë‚´ íŒŒíŠ¸ì˜ ì¼ë¶€ì¼ ìˆ˜ ìˆìŒ
                # ìš°ì„  ê°„ë‹¨í•˜ê²ŒëŠ” 'ì´ë¯¸ ì²˜ë¦¬ëœ ì›ë³¸ dfì˜ ì¸ë±ìŠ¤'ë¥¼ íŒŒì•…
                processed_indices = set(df_temp[df_temp["ì›ë³¸ê¸°ì‚¬"].notna()].index)
                
                # ê¸°ì¡´ dfì— ë®ì–´ì”Œìš°ê¸° (ì¸ë±ìŠ¤ ë§¤ì¹­)
                df.update(df_temp)
                log(f"ğŸ”„ [Worker {args.worker_id}] ì´ì „ ì‘ì—… ë‚´ì—­ ë°œê²¬: {len(processed_indices)}ê°œ ì²˜ë¦¬ë¨.")
        except Exception as e:
            log(f"âš ï¸ ì„ì‹œ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({e}), ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")

    if "ì›ë³¸ê¸°ì‚¬" not in df.columns:
        df["ì›ë³¸ê¸°ì‚¬"] = ""
        df["ë³µì‚¬ìœ¨"] = 0.0

    total_my_task = len(df)
    log(f"ğŸ”¥ [Worker {args.worker_id}] ë‚´ í• ë‹¹ëŸ‰: {total_my_task}ê°œ (ì „ì²´ {len(df_all)}ê°œ ì¤‘)")

    # ì²˜ë¦¬í•´ì•¼ í•  ì¸ë±ìŠ¤ (ì´ë¯¸ í•œ ê±° ì œì™¸)
    # df.indexëŠ” ì „ì²´ ë°ì´í„° í”„ë ˆì„ì˜ ì›ë³¸ ì¸ë±ìŠ¤ë¥¼ ìœ ì§€í•˜ê³  ìˆìŒ
    target_indices = [i for i in df.index if i not in processed_indices]

    # ë°°ì¹˜ ì²˜ë¦¬
    CHUNK_SIZE = 5 # ì›Œì»¤ë‹¹ ì‘ì—…ëŸ‰ì´ ì¤„ì—ˆìœ¼ë‹ˆ ë” ìì£¼ ì €ì¥í•´ë„ ë¨
    
    for i in range(0, len(target_indices), CHUNK_SIZE):
        chunk_indices = target_indices[i : i + CHUNK_SIZE]
        log(f"ğŸš€ [Worker {args.worker_id}] ë°°ì¹˜ ì‹œì‘ ({i}/{len(target_indices)}) - {len(chunk_indices)}ê±´")

        tasks = [(idx, df.loc[idx].to_dict(), len(df_all)) for idx in chunk_indices]

        # ê° ë¶„í•  íƒœìŠ¤í¬ ì•ˆì—ì„œëŠ” ì›Œì»¤ 1ê°œë§Œ ì‚¬ìš© (ì•ˆì •ì„±)
        with ProcessPoolExecutor(max_workers=1) as executor:
            futures = [executor.submit(find_original_article_multiprocess, *args) for args in tasks]
            
            for future in as_completed(futures):
                try:
                    index, link, score = future.result()
                    df.at[index, "ì›ë³¸ê¸°ì‚¬"] = link
                    df.at[index, "ë³µì‚¬ìœ¨"] = score
                except Exception as e:
                    log(f"âŒ [Worker {args.worker_id}] ê°œë³„ ì˜¤ë¥˜: {e}")

        # ì¤‘ê°„ ì €ì¥
        df.to_csv(temp_output_path, index=True, encoding='utf-8-sig') # ì¸ë±ìŠ¤ í¬í•¨í•´ì„œ ì €ì¥í•´ì•¼ ë‚˜ì¤‘ì— ë§¤ì¹­ ê°€ëŠ¥
        log(f"ğŸ’¾ [Worker {args.worker_id}] ì¤‘ê°„ ì €ì¥ ì™„ë£Œ.")

    # ìµœì¢… ì €ì¥
    storage_options = None
    if args.output_csv.startswith("s3://"):
        storage_options = {"key": AWS_ACCESS_KEY, "secret": AWS_SECRET_KEY}
    
    if not args.output_csv.startswith("s3://"):
        os.makedirs(os.path.dirname(part_output_path), exist_ok=True)

    try:
        # ì¸ë±ìŠ¤ ì—†ì´ ê¹”ë”í•˜ê²Œ ì €ì¥ (ë‚˜ì¤‘ì— DB ì €ì¥ ì‹œ concatí•˜ë©´ ë¨)
        df.to_csv(part_output_path, index=False, encoding='utf-8-sig', storage_options=storage_options)
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if os.path.exists(temp_output_path):
            os.remove(temp_output_path)
            
        log(f"âœ… [Worker {args.worker_id}] ìµœì¢… ì €ì¥ ì™„ë£Œ: {part_output_path}")
    except Exception as e:
        log(f"âŒ [Worker {args.worker_id}] ì €ì¥ ì‹¤íŒ¨: {e}")
        sys.exit(1)